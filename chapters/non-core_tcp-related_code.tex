\chapter{非核心函数分析}

\minitoc

    \section{BSD Socket层}
        \subsection{\mintinline{C}{msg_flag}}
\begin{enumerate}
\item[MSG\_OOB]         接收或发送带外数据。
\item[MSG\_PEEK]        查看数据，并不从系统缓存区移走数据。
\item[MSG\_DONTROUTE]   无需路由查找，目的地位于本地子网。
\item[MSG\_CTRUNC]      指明由于缓存区空间不足，一些控制数据已被丢弃。
\item[MSG\_PROBE]       使用这个标识，实际上并不会进行真正的的数据传递，
                        而是进行路径MTU的探测。
\item[MSG\_TRUNC]       只返回包的真实长度，并截短(丢弃)返回长度的数据。
\item[MSG\_DONTWAIT]    无阻塞接收或发送。如果接收缓存中由数据，则接收数据并立刻返回。
                        没有数据也立刻返回，而不进行任何等待。
\item[MSG\_WAITALL]     必须一直等待，直到接收到的数据填满用户空间的缓存区。
\item[MSG\_CONFIRM]     标识网关有效。只用于SOCK\_DGRAM和SOCK\_RAM类型的套接口。
\item[MSG\_ERRQUEUE]    指示除了来自套接字错误队列的错误外，不接受其他数据。
\item[MSG\_NOSIGNAL]    当另一端终止连接时，请求在基于流的错误套接字上不要发送SIGPIPE信号。
\item[MSG\_MORE]        后续还有数据待发送。
\item[MSG\_CMSG\_COMPAT]64位兼容32位处理方式。
\end{enumerate}
        \subsection{数据报类型}
\begin{minted}[linenos]{C}
/* 
Location:

    include/uapi/linux/if_packet.h

Description:

    Packet types 
*/
#define PACKET_HOST     0           /* To us        */
#define PACKET_BROADCAST    1       /* To all       */
#define PACKET_MULTICAST    2       /* To group     */
#define PACKET_OTHERHOST    3       /* To someone else  */
#define PACKET_OUTGOING     4       /* Outgoing of any type,本机发出的包 */
#define PACKET_LOOPBACK     5       /* MC/BRD frame looped back */
#define PACKET_USER         6       /* To user space    */
#define PACKET_KERNEL       7       /* To kernel space  */
/* Unused, PACKET_FASTROUTE and PACKET_LOOPBACK are invisible to user space */
#define PACKET_FASTROUTE    6       /* Fastrouted frame */

/* Packet socket options */

#define PACKET_ADD_MEMBERSHIP       1
#define PACKET_DROP_MEMBERSHIP      2
#define PACKET_RECV_OUTPUT      3
/* Value 4 is still used by obsolete turbo-packet. */
#define PACKET_RX_RING          5
#define PACKET_STATISTICS       6
#define PACKET_COPY_THRESH      7
#define PACKET_AUXDATA          8
#define PACKET_ORIGDEV          9
#define PACKET_VERSION          10
#define PACKET_HDRLEN           11
#define PACKET_RESERVE          12
#define PACKET_TX_RING          13
#define PACKET_LOSS         14
#define PACKET_VNET_HDR         15
#define PACKET_TX_TIMESTAMP     16
#define PACKET_TIMESTAMP        17
#define PACKET_FANOUT           18
#define PACKET_TX_HAS_OFF       19
#define PACKET_QDISC_BYPASS     20
#define PACKET_ROLLOVER_STATS       21
#define PACKET_FANOUT_DATA      22

#define PACKET_FANOUT_HASH      0
#define PACKET_FANOUT_LB        1
#define PACKET_FANOUT_CPU       2
#define PACKET_FANOUT_ROLLOVER      3
#define PACKET_FANOUT_RND       4
#define PACKET_FANOUT_QM        5
#define PACKET_FANOUT_CBPF      6
#define PACKET_FANOUT_EBPF      7
#define PACKET_FANOUT_FLAG_ROLLOVER 0x1000
#define PACKET_FANOUT_FLAG_DEFRAG   0x8000
\end{minted}    
        \subsection{Sock CheckSum}
            \subsubsection{CheckSum相关标志位}
\begin{minted}[linenos]{C}
* A. Checksumming of received packets by device.
 *
 * CHECKSUM_NONE:
 *
 *   Device failed to checksum this packet e.g. due to lack of capabilities.
 *   The packet contains full (though not verified) checksum in packet but
 *   not in skb->csum. Thus, skb->csum is undefined in this case.
 *
 * CHECKSUM_UNNECESSARY:
 *
 *   The hardware you're dealing with doesn't calculate the full checksum
 *   (as in CHECKSUM_COMPLETE), but it does parse headers and verify checksums
 *   for specific protocols. For such packets it will set CHECKSUM_UNNECESSARY
 *   if their checksums are okay. skb->csum is still undefined in this case
 *   though. It is a bad option, but, unfortunately, nowadays most vendors do
 *   this. Apparently with the secret goal to sell you new devices, when you
 *   will add new protocol to your host, f.e. IPv6 8)
 *
 *   CHECKSUM_UNNECESSARY is applicable to following protocols:
 *     TCP: IPv6 and IPv4.
 *     UDP: IPv4 and IPv6. A device may apply CHECKSUM_UNNECESSARY to a
 *       zero UDP checksum for either IPv4 or IPv6, the networking stack
 *       may perform further validation in this case.
 *     GRE: only if the checksum is present in the header.
 *     SCTP: indicates the CRC in SCTP header has been validated.
 *
 *   skb->csum_level indicates the number of consecutive checksums found in
 *   the packet minus one that have been verified as CHECKSUM_UNNECESSARY.
 *   For instance if a device receives an IPv6->UDP->GRE->IPv4->TCP packet
 *   and a device is able to verify the checksums for UDP (possibly zero),
 *   GRE (checksum flag is set), and TCP-- skb->csum_level would be set to
 *   two. If the device were only able to verify the UDP checksum and not
 *   GRE, either because it doesn't support GRE checksum of because GRE
 *   checksum is bad, skb->csum_level would be set to zero (TCP checksum is
 *   not considered in this case).
 *
 * CHECKSUM_COMPLETE:
 *
 *   This is the most generic way. The device supplied checksum of the _whole_
 *   packet as seen by netif_rx() and fills out in skb->csum. Meaning, the
 *   hardware doesn't need to parse L3/L4 headers to implement this.
 *
 *   Note: Even if device supports only some protocols, but is able to produce
 *   skb->csum, it MUST use CHECKSUM_COMPLETE, not CHECKSUM_UNNECESSARY.
 *
 * CHECKSUM_PARTIAL:
 *
 *   A checksum is set up to be offloaded to a device as described in the
 *   output description for CHECKSUM_PARTIAL. This may occur on a packet
 *   received directly from another Linux OS, e.g., a virtualized Linux kernel
 *   on the same host, or it may be set in the input path in GRO or remote
 *   checksum offload. For the purposes of checksum verification, the checksum
 *   referred to by skb->csum_start + skb->csum_offset and any preceding
 *   checksums in the packet are considered verified. Any checksums in the
 *   packet that are after the checksum being offloaded are not considered to
 *   be verified.
 *
 * B. Checksumming on output.
 *
 * CHECKSUM_NONE:
 *
 *   The skb was already checksummed by the protocol, or a checksum is not
 *   required.
 *
 * CHECKSUM_PARTIAL:
 *
 *   The device is required to checksum the packet as seen by hard_start_xmit()
 *   from skb->csum_start up to the end, and to record/write the checksum at
 *   offset skb->csum_start + skb->csum_offset.
 *
 *   The device must show its capabilities in dev->features, set up at device
 *   setup time, e.g. netdev_features.h:
 *
 *  NETIF_F_HW_CSUM - It's a clever device, it's able to checksum everything.
 *  NETIF_F_IP_CSUM - Device is dumb, it's able to checksum only TCP/UDP over
 *            IPv4. Sigh. Vendors like this way for an unknown reason.
 *            Though, see comment above about CHECKSUM_UNNECESSARY. 8)
 *  NETIF_F_IPV6_CSUM - About as dumb as the last one but does IPv6 instead.
 *  NETIF_F_...     - Well, you get the picture.
 *
 * CHECKSUM_UNNECESSARY:
 *
 *   Normally, the device will do per protocol specific checksumming. Protocol
 *   implementations that do not want the NIC(Network Interface Controller,网卡) 
     to perform the checksum calculation should use this flag in their outgoing skbs.
 *
 *  NETIF_F_FCOE_CRC - This indicates that the device can do FCoE FC CRC
 *             offload. Correspondingly, the FCoE protocol driver
 *             stack should use CHECKSUM_UNNECESSARY.
 *
 * Any questions? No questions, good.       --ANK
 */

/* Don't change this without changing skb_csum_unnecessary! */
#define CHECKSUM_NONE       0
#define CHECKSUM_UNNECESSARY    1
#define CHECKSUM_COMPLETE   2
#define CHECKSUM_PARTIAL    3
\end{minted}
            \subsubsection{\mintinline{C}{skb_csum_unnecessary}}
\begin{minted}[linenos]{C}
/*
Location:

    include/linux/skbuff.h

Parameter:

    skb:传输控制块缓存
*/
static inline int skb_csum_unnecessary(const struct sk_buff *skb)
{
    return ((skb->ip_summed == CHECKSUM_UNNECESSARY) ||
        skb->csum_valid ||
        (skb->ip_summed == CHECKSUM_PARTIAL &&
         skb_checksum_start_offset(skb) >= 0));
}
\end{minted}
            如果不想要网卡帮忙校验或者Checksum合法，或者硬件给出的时部分校验并且校验的偏移必须大于零。
            则返回1,否则，返回0。
            \subsubsection{\mintinline{C}{__skb_checksum_complete}}
\begin{minted}[linenos]{C}
__sum16 __skb_checksum_complete(struct sk_buff *skb)
{
    __wsum csum;
    __sum16 sum;

    csum = skb_checksum(skb, 0, skb->len, 0);

    /* skb->csum holds pseudo checksum */
    sum = csum_fold(csum_add(skb->csum, csum));
    if (likely(!sum)) {
        if (unlikely(skb->ip_summed == CHECKSUM_COMPLETE) &&
            !skb->csum_complete_sw)
            netdev_rx_csum_fault(skb->dev);
    }

    if (!skb_shared(skb)) {
        /* Save full packet checksum */
        skb->csum = csum;
        skb->ip_summed = CHECKSUM_COMPLETE;
        skb->csum_complete_sw = 1;
        skb->csum_valid = !sum;
    }

    return sum;
}
\end{minted}    
        \subsection{SK Stream}
Linux内核中，提供了一套通用的处理网络数据流的函数。这一套函数是为了共享各种协议中，
和处理数据流相关的代码，以实现代码重用。
\subsection{\mintinline{c}{sk_stream_wait_connect}}
该函数用于等待套接字完成连接。
\begin{minted}[linenos]{c}
/* Location: net/core/stream.c
 *
 * Parameter:
 *     sk: 套接字
 *     timeo_p: 等待多长时间
 *
 * 该函数必须在sk被上锁的情况下被调用。
 */
int sk_stream_wait_connect(struct sock *sk, long *timeo_p)
{
        struct task_struct *tsk = current;
        DEFINE_WAIT(wait);
        int done;

        do {
                int err = sock_error(sk);
                if (err)
                        return err;
                if ((1 << sk->sk_state) & ~(TCPF_SYN_SENT | TCPF_SYN_RECV))
                        return -EPIPE;
                if (!*timeo_p)
                        return -EAGAIN;
                if (signal_pending(tsk))
                        return sock_intr_errno(*timeo_p);

                prepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);
                sk->sk_write_pending++;
                /* 等待TCP进入连接建立状态 */
                done = sk_wait_event(sk, timeo_p,
                                     !sk->sk_err &&
                                     !((1 << sk->sk_state) &
                                       ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT)));
                finish_wait(sk_sleep(sk), &wait);
                sk->sk_write_pending--;
        } while (!done);
        return 0;
}
EXPORT_SYMBOL(sk_stream_wait_connect);
\end{minted}

\section{Inet}

\subsection{\mintinline{c}{__inet_stream_connect}}
这个函数是用于连接到一个远程的主机的。
\begin{minted}[linenos]{c}
/* Location: net/ipv4/af_inet.c
 *
 * Function: Connect to a remote host. There is regrettably still a little
 *           TCP 'magic' in here.
 *
 * Parameter: 
 *           sock：发起连接的套接字
 *           uaddr：目标地址
 *           addr_len：sockaddr结构体的长度
 *           flags：标志位
 */
int __inet_stream_connect(struct socket *sock, struct sockaddr *uaddr,
                          int addr_len, int flags)
{
        struct sock *sk = sock->sk;
        int err;
        long timeo;

        /* 判断长度和协议 */
        if (addr_len < sizeof(uaddr->sa_family))
                return -EINVAL;

        if (uaddr->sa_family == AF_UNSPEC) {
                err = sk->sk_prot->disconnect(sk, flags);
                sock->state = err ? SS_DISCONNECTING : SS_UNCONNECTED;
                goto out;
        }

        /* 根据当前的状态执行相应的功能
         *     处于已连接或者其他状态返回错误值。
         *     如果处于未连接状态则调用传输层的连接函数，实现连接远程主机的功能。
         */
        switch (sock->state) {
        default:
                err = -EINVAL;
                goto out;
        case SS_CONNECTED:
                err = -EISCONN;
                goto out;
        case SS_CONNECTING:
                err = -EALREADY;
                /* Fall out of switch with err, set for this state */
                break;
        case SS_UNCONNECTED:
                err = -EISCONN;
                if (sk->sk_state != TCP_CLOSE)
                        goto out;

                err = sk->sk_prot->connect(sk, uaddr, addr_len);
                if (err < 0)
                        goto out;

                sock->state = SS_CONNECTING;

                /* Just entered SS_CONNECTING state; the only
                 * difference is that return value in non-blocking
                 * case is EINPROGRESS, rather than EALREADY.
*/
                err = -EINPROGRESS;
                break;
        }

        timeo = sock_sndtimeo(sk, flags & O_NONBLOCK);

        if ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV)) {
                int writebias = (sk->sk_protocol == IPPROTO_TCP) &&
                                tcp_sk(sk)->fastopen_req &&
                                tcp_sk(sk)->fastopen_req->data ? 1 : 0;

                /* Error code is set above */
                if (!timeo || !inet_wait_for_connect(sk, timeo, writebias))
                        goto out;

                err = sock_intr_errno(timeo);
                if (signal_pending(current))
                        goto out;
        }

        /* Connection was closed by RST, timeout, ICMP error
         * or another process disconnected us.
         */
        if (sk->sk_state == TCP_CLOSE)
                goto sock_error;

        /* sk->sk_err may be not zero now, if RECVERR was ordered by user
         * and error was received after socket entered established state.
         * Hence, it is handled normally after connect() return successfully.
         */

        sock->state = SS_CONNECTED;
        err = 0;
out:
        return err;

sock_error:
        err = sock_error(sk) ? : -ECONNABORTED;
        sock->state = SS_UNCONNECTED;
        if (sk->sk_prot->disconnect(sk, flags))
                sock->state = SS_DISCONNECTING;
        goto out;
}
\end{minted}
    
\subsection{\mintinline{C}{inet_hash_connect && __inet_hash_connect}}

\subsubsection{\mintinline{C}{inet_hash_connect}}
\begin{minted}[linenos]{C}
/*
 * Bind a port for a connect operation and hash it.
 */
int inet_hash_connect(struct inet_timewait_death_row *death_row,
              struct sock *sk)
{
    u32 port_offset = 0;

    if (!inet_sk(sk)->inet_num)
        port_offset = inet_sk_port_offset(sk);
    return __inet_hash_connect(death_row, sk, port_offset,
                   __inet_check_established);
}
\end{minted}

        \subsubsection{\mintinline{C}{__inet_hash_connect}}     
\begin{minted}[linenos]{C}
int __inet_hash_connect(struct inet_timewait_death_row *death_row,
        struct sock *sk, u32 port_offset,
        int (*check_established)(struct inet_timewait_death_row *,
            struct sock *, __u16, struct inet_timewait_sock **))
{
    struct inet_hashinfo *hinfo = death_row->hashinfo;
    const unsigned short snum = inet_sk(sk)->inet_num;
    struct inet_bind_hashbucket *head;
    struct inet_bind_bucket *tb;
    int ret;
    struct net *net = sock_net(sk);

    if (!snum) {
        int i, remaining, low, high, port;
        static u32 hint;
        u32 offset = hint + port_offset;
        struct inet_timewait_sock *tw = NULL;

        inet_get_local_port_range(net, &low, &high);
        remaining = (high - low) + 1;

        /* By starting with offset being an even number,
         * we tend to leave about 50% of ports for other uses,
         * like bind(0).
         */
        offset &= ~1;

        local_bh_disable();
        for (i = 0; i < remaining; i++) {
            port = low + (i + offset) % remaining;
            if (inet_is_local_reserved_port(net, port))
                continue;
            head = &hinfo->bhash[inet_bhashfn(net, port,
                    hinfo->bhash_size)];
            spin_lock(&head->lock);

            /* Does not bother with rcv_saddr checks,
             * because the established check is already
             * unique enough.
             */
            inet_bind_bucket_for_each(tb, &head->chain) {
                if (net_eq(ib_net(tb), net) &&
                    tb->port == port) {
                    if (tb->fastreuse >= 0 ||
                        tb->fastreuseport >= 0)
                        goto next_port;
                    WARN_ON(hlist_empty(&tb->owners));
                    if (!check_established(death_row, sk,
                                port, &tw))
                        goto ok;
                    goto next_port;
                }
            }

            tb = inet_bind_bucket_create(hinfo->bind_bucket_cachep,
                    net, head, port);
            if (!tb) {
                spin_unlock(&head->lock);
                break;
            }
            tb->fastreuse = -1;
            tb->fastreuseport = -1;
            goto ok;

        next_port:
            spin_unlock(&head->lock);
        }
        local_bh_enable();

        return -EADDRNOTAVAIL;

ok:
        hint += (i + 2) & ~1;

        /* Head lock still held and bh's disabled */
        inet_bind_hash(sk, tb, port);
        if (sk_unhashed(sk)) {
            inet_sk(sk)->inet_sport = htons(port);
            inet_ehash_nolisten(sk, (struct sock *)tw);
        }
        if (tw)
            inet_twsk_bind_unhash(tw, hinfo);
        spin_unlock(&head->lock);

        if (tw)
            inet_twsk_deschedule_put(tw);

        ret = 0;
        goto out;
    }

    head = &hinfo->bhash[inet_bhashfn(net, snum, hinfo->bhash_size)];
    tb  = inet_csk(sk)->icsk_bind_hash;
    spin_lock_bh(&head->lock);
    if (sk_head(&tb->owners) == sk && !sk->sk_bind_node.next) {
        inet_ehash_nolisten(sk, NULL);
        spin_unlock_bh(&head->lock);
        return 0;
    } else {
        spin_unlock(&head->lock);
        /* No definite answer... Walk to established hash table */
        ret = check_established(death_row, sk, snum, NULL);
out:
        local_bh_enable();
        return ret;
    }
}
\end{minted}
    \subsection{\mintinline{C}{inet_csk_reqsk_queue_add}}
        \label{INET:inet_csk_reqsk_queue_add}
\begin{minted}[linenos]{C}
/*
Location:

*/
struct sock *inet_csk_reqsk_queue_add(struct sock *sk,
                      struct request_sock *req,
                      struct sock *child)
{
    struct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;

    spin_lock(&queue->rskq_lock);
    if (unlikely(sk->sk_state != TCP_LISTEN)) {
        inet_child_forget(sk, req, child);
        child = NULL;
    } else {
        req->sk = child;
        req->dl_next = NULL;
        if (queue->rskq_accept_head == NULL)
            queue->rskq_accept_head = req;
        else
            queue->rskq_accept_tail->dl_next = req;
        queue->rskq_accept_tail = req;
        sk_acceptq_added(sk);
    }
    spin_unlock(&queue->rskq_lock);
    return child;
}
\end{minted}
        这一个函数所进行的操作就是直接将请求挂在接收队列中。

        \textcolor{blue}{调用位置}:

            \begin{enumerate}
                \item[1]        \ref{Server:tcp_conn_request}中的tcp\_conn\_request调用。
            \end{enumerate}

        \subsubsection{\mintinline{C}{inet_csk_reqsk_queue_hash_add}}
            \label{INET:inet_csk_reqsk_queue_hash_add}
\begin{minted}[linenos]{C}
void inet_csk_reqsk_queue_hash_add(struct sock *sk, struct request_sock *req,
                   unsigned long timeout)
{
    reqsk_queue_hash_req(req, timeout);
    inet_csk_reqsk_queue_added(sk);
}
\end{minted}

        首先将连接请求块保存到父传输请求块的散列表中，并设置定时器超时时间。之后更新已存在的连接请求块数，并启动连接建立定时器。

        \textcolor{blue}{调用位置}:

            \begin{enumerate}
                \item[1]        \ref{Server:tcp_conn_request}中的tcp\_conn\_request调用。
            \end{enumerate}

    \subsection{\mintinline{c}{inet_twsk_put}}
该函数用于释放\mintinline{c}{inet_timewait_sock}结构体。
\begin{minted}[linenos]{c}
void inet_twsk_put(struct inet_timewait_sock *tw)
{
        /* 减小引用计数，如果计数为0，则释放它 */
        if (atomic_dec_and_test(&tw->tw_refcnt))
                inet_twsk_free(tw);
}
\end{minted}

\section{TCP层}

    \mintinline{c}{TCP_INC_STATS_BH}

    \mintinline{c}{rcu_read_unlock} 出现在\mintinline{c}{tcp_make_synack}中于MD5相关的部分。
    
    \mintinline{c}{net_xmit_eval}   定时器？？
    \subsection{TCP相关参数}
\begin{enumerate}
\item[tcp\_syn\_retries]  INTEGER,
默认值是5
对于一个新建连接，内核要发送多少个 SYN 连接请求才决定放弃。不应该大于255，默认值是5，对应于180秒左右时间。(对于大负载而物理通信良好的网络而言,这个值偏高,可修改为2.这个值仅仅是针对对外的连接,对进来的连接,是由
tcp\_retries1 决定的)

\item[tcp\_synack\_retries] INTEGER,
默认值是5
对于远端的连接请求SYN，内核会发送SYN ＋ ACK数据报，以确认收到上一个 SYN连接请求包。这是所谓的三次握手( threeway handshake)机制的第二个步骤。这里决定内核在放弃连接之前所送出的 SYN+ACK 数目。不应该大于255，默认值是5，对应于180秒左右时间。(可以根据上面的tcp\_syn\_retries来决定这个值)

\item[tcp\_keepalive\_time] INTEGER,
默认值是7200(2小时)
当keepalive打开的情况下，TCP发送keepalive消息的频率。(由于目前网络攻击等因素,造成了利用这个进行的攻击很频繁,曾经也有cu的朋友提到过,说如果2边建立了连接,然后不发送任何数据或者rst/fin消息,那么持续的时间是不是就是2小时,空连接攻击?tcp\_keepalive\_time就是预防此情形的.我个人在做nat服务的时候的修改值为1800秒)

\item[tcp\_keepalive\_probes] INTEGER,
默认值是9
TCP发送keepalive探测以确定该连接已经断开的次数。(注意:保持连接仅在SO\_KEEPALIVE套接字选项被打开是才发送.次数默认不需要修改,当然根据情形也可以适当地缩短此值.设置为5比较合适)

\item[tcp\_keepalive\_intvl] INTEGER,
默认值为75
探测消息发送的频率，乘以tcp\_keepalive\_probes就得到对于从开始探测以来没有响应的连接杀除的时间。默认值为75秒，也就是没有活动的连接将在大约11分钟以后将被丢弃。(对于普通应用来说,这个值有一些偏大,可以根据需要改小.特别是web类服务器需要改小该值,15是个比较合适的值)

\item[tcp\_retries1] INTEGER,
默认值是3
放弃回应一个TCP连接请求前﹐需要进行多少次重试。RFC 规定最低的数值是3﹐这也是默认值﹐根据RTO的值大约在3秒 - 8分钟之间。(注意:这个值同时还决定进入的syn连接)

\item[tcp\_retries2] INTEGER,
默认值为15
在丢弃激活(已建立通讯状况)的TCP连接之前﹐需要进行多少次重试。默认值为15，根据RTO的值来决定，相当于13-30分钟(RFC1122规定，必须大于100秒).(这个值根据目前的网络设置,可以适当地改小,我的网络内修改为了5)

\item[tcp\_orphan\_retries] INTEGER,
默认值是7
在近端丢弃TCP连接之前﹐要进行多少次重试。默认值是7个﹐相当于 50秒 - 16分钟﹐视 RTO 而定。如果您的系统是负载很大的web服务器﹐那么也许需要降低该值﹐这类 sockets 可能会耗费大量的资源。另外参的考tcp\_max\_orphans 。(事实上做NAT的时候,降低该值也是好处显著的,我本人的网络环境中降低该值为3)

\item[tcp\_fin\_timeout] INTEGER,
默认值是 60
对于本端断开的socket连接，TCP保持在FIN-WAIT-2状态的时间。对方可能会断开连接或一直不结束连接或不可预料的进程死亡。默认值为 60 秒。过去在2.2版本的内核中是 180 秒。您可以设置该值﹐但需要注意﹐如果您的机器为负载很重的web服务器﹐您可能要冒内存被大量无效数据报填满的风险﹐FIN-WAIT-2 sockets 的危险性低于 FIN-WAIT-1 ﹐因为它们最多只吃 1.5K 的内存﹐但是它们存在时间更长。另外参考 tcp\_max\_orphans。(事实上做NAT的时候,降低该值也是好处显著的,我本人的网络环境中降低该值为30)

\item[tcp\_max\_tw\_buckets] INTEGER,
默认值是180000
系统在同时所处理的最大 timewait sockets 数目。如果超过此数的话﹐time-wait socket 会被立即砍除并且显示警告信息。之所以要设定这个限制﹐纯粹为了抵御那些简单的 DoS 攻击﹐千万不要人为的降低这个限制﹐不过﹐如果网络条件需要比默认值更多﹐则可以提高它(或许还要增加内存)。(事实上做NAT的时候最好可以适当地增加该值)

\item[tcp\_tw\_recycle] BOOLEAN,
默认值是0
打开快速 TIME-WAIT sockets 回收。除非得到技术专家的建议或要求﹐请不要随意修改这个值。(做NAT的时候，建议打开它)

\item[tcp\_tw\_reuse] BOOLEAN,
默认值是0
该文件表示是否允许重新应用处于TIME-WAIT状态的socket用于新的TCP连接(这个对快速重启动某些服务,而启动后提示端口已经被使用的情形非常有帮助)

\item[tcp\_max\_orphans] INTEGER,
缺省值是8192
系统所能处理不属于任何进程的TCP sockets最大数量。假如超过这个数量﹐那么不属于任何进程的连接会被立即reset，并同时显示警告信息。之所以要设定这个限制﹐纯粹为了抵御那些简单的 DoS 攻击﹐千万不要依赖这个或是人为的降低这个限制(这个值Redhat AS版本中设置为32768,但是很多防火墙修改的时候,建议该值修改为2000)

\item[tcp\_abort\_on\_overflow] BOOLEAN,
缺省值是0
当守护进程太忙而不能接受新的连接，就象对方发送reset消息，默认值是false。这意味着当溢出的原因是因为一个偶然的猝发，那么连接将恢复状态。只有在你确信守护进程真的不能完成连接请求时才打开该选项，该选项会影响客户的使用。(对待已经满载的sendmail,apache这类服务的时候,这个可以很快让客户端终止连接,可以给予服务程序处理已有连接的缓冲机会,所以很多防火墙上推荐打开它)

\item[tcp\_syncookies] BOOLEAN,
默认值是0
只有在内核编译时选择了CONFIG\_SYNCOOKIES时才会发生作用。当出现syn等候队列出现溢出时象对方发送syncookies。目的是为了防止syn flood攻击。
注意：该选项千万不能用于那些没有收到攻击的高负载服务器，如果在日志中出现synflood消息，但是调查发现没有收到synflood攻击，而是合法用户的连接负载过高的原因，你应该调整其它参数来提高服务器性能。参考:
    tcp\_max\_syn\_backlog、
    tcp\_synack\_retries、
    tcp\_abort\_on\_overflow。
syncookie严重的违背TCP协议，不允许使用TCP扩展，可能对某些服务导致严重的性能影响(如SMTP转发)。(注意,该实现与BSD上面使用的tcp proxy一样,是违反了RFC中关于tcp连接的三次握手实现的,但是对于防御syn-flood的确很有用.)

\item[tcp\_stdurg] BOOLEAN,
默认值为0
使用 TCP urg pointer 字段中的主机请求解释功能。大部份的主机都使用老旧的 BSD解释，因此如果您在 Linux 打开它﹐或会导致不能和它们正确沟通。

\item[tcp\_max\_syn\_backlog] INTEGER,
对于那些依然还未获得客户端确认的连接请求﹐需要保存在队列中最大数目。对于超过 128Mb 内存的系统﹐默认值是1024 ﹐低于 128Mb 的则为 128。如果服务器经常出现过载﹐可以尝试增加这个数字。警告﹗假如您将此值设为大于1024﹐最好修改 include/net/tcp.h 里面的 TCP\_SYNQ\_HSIZE ﹐以保持TCP\_SYNQ\_HSIZE*16<=tcp\_max\_syn\_backlog ﹐并且编进核心之内。(SYN Flood攻击利用TCP协议散布握手的缺陷，伪造虚假源IP地址发送大量TCP-SYN半打开连接到目标系统，最终导致目标系统Socket队列资源耗尽而无法接受新的连接。为了应付这种攻击，现代Unix系统中普遍采用多连接队列处理的方式来缓冲(而不是解决)这种攻击，是用一个基本队列处理正常的完全连接应用(Connect()和Accept() )，是用另一个队列单独存放半打开连接。这种双队列处理方式和其他一些系统内核措施(例如Syn-Cookies/Caches)联合应用时，能够比较有效的缓解小规模的SYN Flood攻击(事实证明<1000p/s)加大SYN队列长度可以容纳更多等待连接的网络连接数，所以对Server来说可以考虑增大该值.)

\item[tcp\_window\_scaling] BOOL,
                            缺省值为1,
                            该标志表示tcp/ip会话的滑动窗口大小是否可变。参数值为布尔值，为1时表示可变，为0时表示不可变。tcp/ip通常使用的窗口最大可达到 65535 字节，对于高速网络，该值可能太小，这时候如果启用了该功能，可以使tcp/ip滑动窗口大小增大数个数量级，从而提高数据传输的能力(RFC 1323)。当然，对普通的百M网络而言，关闭会降低开销，所以如果不是高速网络，可以设置为0。

\item[tcp\_timestamps] BOOLEAN,
                        缺省值为1，
                        该标志表示是否启用以一种比超时重发更精确的方法（RFC 1323）来启用对 RTT 的计算，为了实现更好的性能应该启用这个选项。同时，Timestamps可以防范那些伪造的 sequence 序列号，一条1G的宽带线路或许会重遇到带 out-of-line数值的旧sequence 号码(假如它是由于上次产生的)。Timestamp 会让它知道这是个 '旧封包'。

\item[tcp\_sack] BOOLEAN,
                缺省值为1，
                该标志表示是否启用有选择的应答（Selective Acknowledgment），这可以通过有选择地应答乱序接收到的报文来提高性能（这样可以让发送者只发送丢失的报文段）。它可以用来查找特定的遗失的数据报，因此有助于快速恢复状态。对于广域网通信来说这个选项应该启用，但是这会增加对 CPU 的占用。

\item[tcp\_fack] BOOLEAN,
缺省值为1
打开FACK拥塞避免和快速重传功能。(注意，当tcp\_sack设置为0的时候，这个值即使设置为1也无效)

\item[tcp\_dsack] BOOLEAN,
缺省值为1
允许TCP发送"两个完全相同"的SACK。

\item[tcp\_ecn] BOOLEAN,
缺省值为0
打开TCP的直接拥塞通告功能。

\item[tcp\_reordering] INTEGER,
默认值是3
TCP流中重排序的数据报最大数量 。 (一般有看到推荐把这个数值略微调整大一些,比如5)

\item[tcp\_retrans\_collapse] BOOLEAN，
缺省值为1
对于某些有bug的打印机提供针对其bug的兼容性。(一般不需要这个支持,可以关闭它)

\item[tcp\_wmem(3个INTEGER变量)] min, default, max。
min：为TCP socket预留用于发送缓冲的内存最小值。每个tcp socket都可以在建议以后都可以使用它。默认值为4096(4K)。

default：为TCP socket预留用于发送缓冲的内存数量，默认情况下该值会影响其它协议使用的net.core.wmem\_default 值，一般要低于net.core.wmem\_default的值。默认值为16384(16K)。

max: 用于TCP socket发送缓冲的内存最大值。该值不会影响net.core.wmem\_max，"静态"选择参数SO\_SNDBUF则不受该值影响。默认值为131072(128K)。（对于服务器而言，增加这个参数的值对于发送数据很有帮助,在我的网络环境中,修改为了51200 131072 204800）

\item[tcp\_rmem ] (3个INTEGER变量)min, default, max。
min：为TCP socket预留用于接收缓冲的内存数量，即使在内存出现紧张情况下tcp socket都至少会有这么多数量的内存用于接收缓冲，默认值为8K。

default：为TCP socket预留用于接收缓冲的内存数量，默认情况下该值影响其它协议使用的net.core.wmem\_default 值。该值决定了在tcp\_adv\_win\_scale、tcp\_app\_win和tcp\_app\_win=0默认值情况下，TCP窗口大小为65535。默认值为87380

max：用于TCP socket接收缓冲的内存最大值。该值不会影响 net.core.wmem\_max，"静态"选择参数 SO\_SNDBUF则不受该值影响。默认值为 128K。默认值为87380*2 bytes。（可以看出，.max的设置最好是default的两倍,对于NAT来说主要该增加它,我的网络里为 51200 131072 204800）

\item[tcp\_mem] (3个INTEGER变量)low, pressure, high。
                    low：当TCP使用了低于该值的内存页面数时，TCP不会考虑释放内存。(理想情况下，这个值应与指定给 tcp\_wmem 的第 2 个值相匹配 - 这第 2 个值表明，最大页面大小乘以最大并发请求数除以页大小 (131072 * 300 / 4096)。 )

                    pressure：当TCP使用了超过该值的内存页面数量时，TCP试图稳定其内存使用，进入pressure模式，当内存消耗低于low值时则退出pressure状态。(理想情况下这个值应该是 TCP 可以使用的总缓冲区大小的最大值 (204800 * 300 / 4096)。 )

                    high：允许所有tcp sockets用于排队缓冲数据报的页面量。(如果超过这个值，TCP 连接将被拒绝，这就是为什么不要令其过于保守 (512000 * 300 / 4096) 的原因了。 在这种情况下，提供的价值很大，它能处理很多连接，是所预期的 2.5 倍；或者使现有连接能够传输 2.5 倍的数据。 我的网络里为192000 300000 732000)

一般情况下这些值是在系统启动时根据系统内存数量计算得到的。

\item[tcp\_app\_win] INTEGER,
默认值是31
保留max($window/2^{tcp\_app\_win}$, mss)数量的窗口由于应用缓冲。当为0时表示不需要缓冲。

\item[tcp\_adv\_win\_scale] INTEGER,
默认值为2
计算缓冲开销$bytes/2^{tcp\_adv\_win\_scale}$(如果tcp\_adv\_win\_scale > 0)或者$bytes-bytes/2^{-tcp\_adv\_win\_scale}$(如果tcp\_adv\_win\_scale <= 0）。

\item[tcp\_rfc1337] BOOLEAN,
缺省值为0
这个开关可以启动对于在RFC1337中描述的"tcp 的time-wait暗杀危机"问题的修复。启用后，内核将丢弃那些发往time-wait状态TCP套接字的RST 包.

\item[tcp\_low\_latency]    BOOLEAN,缺省值为0。该段的英文阐述如下：If set, the TCP stack makes decisions that prefer lower latency as opposed to higher throughput. By default, this option is not set meaning that higher throughput is preferred. An example of an application where this default should be changed would be a Beowulf compute cluster. 可以看出该选项在设置的时候主要是用于高吞吐量的环境，而设置之后，则是主要用于低延时的环境。这个选项一般情形是的禁用，但在构建Beowulf 集群(超算)的时候,打开它很有帮助。

\item[tcp\_westwood] BOOLEAN,
                    缺省值为0,
                    启用发送者端的拥塞控制算法，它可以维护对吞吐量的评估，并试图对带宽的整体利用情况进行优化；对于 WAN 通信来说应该启用这个选项。

\item[tcp\_bic] BOOLEAN,
                缺省值为0,
                为快速长距离网络启用 Binary Increase Congestion；这样可以更好地利用以 GB 速度进行操作的链接；对于 WAN 通信应该启用这个选项。
\end{enumerate}
    \subsection{TCP 标志宏}
\begin{minted}[linenos]{C}
#define FLAG_DATA       0x01        /* Incoming frame contained data.       */
#define FLAG_WIN_UPDATE     0x02    /* Incoming ACK was a window update.    */
#define FLAG_DATA_ACKED     0x04    /* This ACK acknowledged new data.      */
#define FLAG_RETRANS_DATA_ACKED 0x08 /* "" "" some of which was retransmitted.  */
#define FLAG_SYN_ACKED      0x10    /* This ACK acknowledged SYN.       */
#define FLAG_DATA_SACKED    0x20    /* New SACK.                */
#define FLAG_ECE        0x40        /* ECE in this ACK              */
#define FLAG_LOST_RETRANS   0x80 /* This ACK marks some retransmission lost */
#define FLAG_SLOWPATH       0x100 /* Do not skip RFC checks for window update.*/
#define FLAG_ORIG_SACK_ACKED    0x200 /* Never retransmitted data are (s)acked  */
#define FLAG_SND_UNA_ADVANCED   0x400 /* Snd_una was changed (!= FLAG_DATA_ACKED) */
#define FLAG_DSACKING_ACK   0x800 /* SACK blocks contained D-SACK info */
#define FLAG_SACK_RENEGING  0x2000 /* snd_una advanced to a sacked seq */
#define FLAG_UPDATE_TS_RECENT   0x4000 /* tcp_replace_ts_recent() */

#define FLAG_ACKED      (FLAG_DATA_ACKED|FLAG_SYN_ACKED)
#define FLAG_NOT_DUP        (FLAG_DATA|FLAG_WIN_UPDATE|FLAG_ACKED)
#define FLAG_CA_ALERT       (FLAG_DATA_SACKED|FLAG_ECE)
#define FLAG_FORWARD_PROGRESS   (FLAG_ACKED|FLAG_DATA_SACKED)

#define TCP_REMNANT (TCP_FLAG_FIN|TCP_FLAG_URG|TCP_FLAG_SYN|TCP_FLAG_PSH)
#define TCP_HP_BITS (~(TCP_RESERVED_BITS|TCP_FLAG_PSH))
\end{minted}

            这里我们需要介绍几个相关的概念，如下：

\begin{enumerate}
\item[SACK]         Selective Acknowledgment(SACK)，这种方式需要在TCP头里加一个SACK的东西，ACK还是Fast Retransmit的ACK，
                    SACK则是汇报收到的数据碎版。这样，在发送端就可以根据回传的SACK来知道哪些数据到了，哪些没有到。
                    于是就优化了 Fast Retransmit的算法。当然，这个协议需要两边都支持。
\item[Reneging]     所谓Reneging的意思就是违约，接收方有权把已经报给发送端SACK里的数据给丢了。
                    当然，我们肯定是不鼓励这样做的，因为这个事会把问题复杂化。
                    但是，接收方可能会由于一些极端情况这么做，比如要把内存给别的更重要的东西。
                    所以，发送方也不能完全依赖SACK，主要还是要依赖ACK，并维护Time-Out。
                    如果后续的ACK没有增长，那么还是要把SACK的东西重传，另外，接收端这边永远不能把SACK的包标记为Ack。
　　                    \color{red}{注意}：SACK会消费发送方的资源，试想，如果一个攻击者给数据发送方发一堆SACK的选项，
                    这会导致发送方开始要重传甚至遍历已经发出的数据，这会消耗很多发送端的资源。

\item[D-SACK]       Duplicate SACK又称D-SACK，其主要使用了SACK来告诉发送方有哪些数据被重复接收了。
                    D-SACK使用了SACK的第一个段来做标志，如果SACK的第一个段的范围被ACK所覆盖，那么就是D-SACK。
                    如果SACK的第一个段的范围被SACK的第二个段覆盖，那么就是D-SACK。引入了D-SACK，有这么几个好处：
　                      1）可以让发送方知道，是发出去的包丢了，还是回来的ACK包丢了。

　　                        2）是不是自己的timeout太小了，导致重传。

　                      3）网络上出现了先发的包后到的情况（又称reordering）

　　                        4）网络上是不是把我的数据包给复制了。
\end{enumerate}
    \subsection{函数宏}
        \subsubsection{\mintinline{C}{before & after}}
            在一些需要判断序号前后的地方出现了\mintinline{c}{before()}和\mintinline{c}{after()}
            这两个函数。这两个函数的定义如下
\begin{minted}[linenos]{c}
/* include/net/tcp.h
 * 比较两个无符号32位整数
 */
static inline bool before(__u32 seq1, __u32 seq2)
{
        return (__s32)(seq1-seq2) < 0;
}
#define after(seq2, seq1)       before(seq1, seq2)
\end{minted}
        可以看到，这两个函数实际上就是将两个数直接相减。之所以要单独弄个函数应该是为了避免强制转型
        造成影响。序号都是32位无符号整型。
        \subsubsection{between}

\begin{minted}[linenos]{C}
/* is s2<=s1<=s3 ? */
static inline bool between(__u32 seq1, __u32 seq2, __u32 seq3)
{
    return seq3 - seq2 >= seq1 - seq2;
}
\end{minted}

    \subsection{TCP CheckSum}
        \subsubsection{\mintinline{C}{tcp_checksum_complete}}
            \ref{TCPCheckSum:tcp_checksum_complete}
            该函数时基于伪首部累加和，完成全包校验和的检测，值得注意的是，该函数用于
            校验没有负载的TCP段。如果需要校验，并且校验成功，则返回1。
\begin{minted}[linenos]{C}
/*
Location:
    include/net/tcp.h
*/
static inline bool tcp_checksum_complete(struct sk_buff *skb)
{
    return !skb_csum_unnecessary(skb) &&
        __tcp_checksum_complete(skb);
}
\end{minted}
            对于\mintinline{C}{__tcp_checksum_complete}函数，如下：
\begin{minted}[linenos]{C}
/*
Location:
    include/net/tcp.h
*/
static inline __sum16 __tcp_checksum_complete(struct sk_buff *skb)
{
    return __skb_checksum_complete(skb);
}
\end{minted}
            对于函数\mintinline{C}{__skb_checksum_complete},参见BSD Socket处的叙述。

            \textcolor{blue}{调用位置}:

                \begin{enumerate}
                    \item[1]        \ref{ServerReceiveSYN:tcp_v4_do_rcv}中的tcp\_v4\_do\_rcv调用。
                \end{enumerate}
            
        \subsubsection{\mintinline{C}{tcp_v4_checksum_init}}
\begin{minted}[linenos]{C}
/*没找到。。*/
\end{minted}
    \subsection{TCP Initialize}
        \subsubsection{tcp\_connnect\_init}
            \label{TCPInitialize:tcp_connect_init}
\begin{minted}[linenos]{C}
/* Do all connect socket setups that can be done AF independent. */
static void tcp_connect_init(struct sock *sk)
{
    const struct dst_entry *dst = __sk_dst_get(sk);
    struct tcp_sock *tp = tcp_sk(sk);
    __u8 rcv_wscale;

    /* We'll fix this up when we get a response from the other end.
     * See tcp_input.c:tcp_rcv_state_process case TCP_SYN_SENT.
     */
    tp->tcp_header_len = sizeof(struct tcphdr) +
        (sysctl_tcp_timestamps ? TCPOLEN_TSTAMP_ALIGNED : 0);

#ifdef CONFIG_TCP_MD5SIG
    if (tp->af_specific->md5_lookup(sk, sk))
        tp->tcp_header_len += TCPOLEN_MD5SIG_ALIGNED;
#endif

    /* If user gave his TCP_MAXSEG, record it to clamp */
    if (tp->rx_opt.user_mss)
        tp->rx_opt.mss_clamp = tp->rx_opt.user_mss;
    tp->max_window = 0;
    tcp_mtup_init(sk);
    tcp_sync_mss(sk, dst_mtu(dst));

    tcp_ca_dst_init(sk, dst);

    if (!tp->window_clamp)
        tp->window_clamp = dst_metric(dst, RTAX_WINDOW);
    tp->advmss = dst_metric_advmss(dst);
    if (tp->rx_opt.user_mss && tp->rx_opt.user_mss < tp->advmss)
        tp->advmss = tp->rx_opt.user_mss;

    tcp_initialize_rcv_mss(sk);

    /* limit the window selection if the user enforce a smaller rx buffer */
    if (sk->sk_userlocks & SOCK_RCVBUF_LOCK &&
        (tp->window_clamp > tcp_full_space(sk) || tp->window_clamp == 0))
        tp->window_clamp = tcp_full_space(sk);

    tcp_select_initial_window(tcp_full_space(sk),
                  tp->advmss - (tp->rx_opt.ts_recent_stamp ? tp->tcp_header_len - sizeof(struct tcphdr) : 0),
                  &tp->rcv_wnd,
                  &tp->window_clamp,
                  sysctl_tcp_window_scaling,
                  &rcv_wscale,
                  dst_metric(dst, RTAX_INITRWND));

    tp->rx_opt.rcv_wscale = rcv_wscale;
    tp->rcv_ssthresh = tp->rcv_wnd;

    sk->sk_err = 0;
    sock_reset_flag(sk, SOCK_DONE);
    tp->snd_wnd = 0;
    tcp_init_wl(tp, 0);
    tp->snd_una = tp->write_seq;
    tp->snd_sml = tp->write_seq;
    tp->snd_up = tp->write_seq;
    tp->snd_nxt = tp->write_seq;

    if (likely(!tp->repair))
        tp->rcv_nxt = 0;
    else
        tp->rcv_tstamp = tcp_time_stamp;
    tp->rcv_wup = tp->rcv_nxt;
    tp->copied_seq = tp->rcv_nxt;

    inet_csk(sk)->icsk_rto = TCP_TIMEOUT_INIT;
    inet_csk(sk)->icsk_retransmits = 0;
    tcp_clear_retrans(tp);
}
\end{minted}

        \textcolor{blue}{调用位置}:

            \begin{enumerate}
            \item[1]        \ref{Client:tcp_connect}的tcp\_connect中调用。
            \end{enumerate}
        \subsubsection{tcp\_init\_nondata\_skb}
            \label{TCPInitialize:tcp_init_nondata_skb}
            该函数提供了初始化不含数据的skb的功能。
\begin{minted}[linenos]{c}
/*
Location:

    net/ipv4/tcp_output.c
    
Function:

    初始化不含数据的skb.

Parameter:

    skb:待初始化的sk_buff。
    seq:序号
    flags:标志位
*/
static void tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags)
{
        /* 设置校验码 */
        skb->ip_summed = CHECKSUM_PARTIAL;
        skb->csum = 0;

        /* 设置标志位 */
        TCP_SKB_CB(skb)->tcp_flags = flags;
        TCP_SKB_CB(skb)->sacked = 0;

        tcp_skb_pcount_set(skb, 1);

        /* 设置起始序号 */
        TCP_SKB_CB(skb)->seq = seq;
        if (flags & (TCPHDR_SYN | TCPHDR_FIN))
                seq++;
        TCP_SKB_CB(skb)->end_seq = seq;
}
\end{minted}

        \textcolor{blue}{调用位置}:

            \begin{enumerate}
                \item[1]        \ref{Client:tcp_connect}中的tcp\_connect调用。
                \item[2]        \ref{Client:tcp_send_ack}中的tcp\_send\_ack调用。
            \end{enumerate}

    \subsection{\mintinline{C}{TCP Options}}
        \subsubsection{TCP Options Flags}

\begin{minted}[linenos]{C}
/*
Location

    include/net/tcp.h
    
Description:
    
    TCP option
*/
#define TCPOPT_NOP          1   /* Padding */
#define TCPOPT_EOL          0   /* End of options */
#define TCPOPT_MSS          2   /* Segment size negotiating */
#define TCPOPT_WINDOW       3   /* Window scaling */
#define TCPOPT_SACK_PERM    4   /* SACK Permitted */
#define TCPOPT_SACK         5   /* SACK Block */
#define TCPOPT_TIMESTAMP    8   /* Better RTT estimations/PAWS */
#define TCPOPT_MD5SIG       19  /* MD5 Signature (RFC2385) */
#define TCPOPT_FASTOPEN     34  /* Fast open (RFC7413) */
#define TCPOPT_EXP          254 /* Experimental */
/* Magic number to be after the option value for sharing TCP
 * experimental options. See draft-ietf-tcpm-experimental-options-00.txt
 */
#define TCPOPT_FASTOPEN_MAGIC   0xF989

/*
 *     TCP option lengths
 */

#define TCPOLEN_MSS            4
#define TCPOLEN_WINDOW         3
#define TCPOLEN_SACK_PERM      2
#define TCPOLEN_TIMESTAMP      10
#define TCPOLEN_MD5SIG         18
#define TCPOLEN_FASTOPEN_BASE  2
#define TCPOLEN_EXP_FASTOPEN_BASE  4
\end{minted}        
        \subsubsection{\mintinline{C}{tcp_parse_fastopen_option}}
            \label{TCPOptions:tcp_parse_fastopen_option}
\begin{minted}[linenos]{C}
/*
Location:

    net/ipv4/tcp_input.c

Function:

    处理Fastopen

Parameter:

    len:长度
    cookie:用于fast open
    syn:是否有syn
    foc:用于fastopen cookie
    exp_opt:是否是实验阶段。
*/
static void tcp_parse_fastopen_option(int len, const unsigned char *cookie,
                      bool syn, struct tcp_fastopen_cookie *foc,
                      bool exp_opt)
{
    /* Valid only in SYN or SYN-ACK with an even length.  */
    if (!foc || !syn || len < 0 || (len & 1))
        return;

    if (len >= TCP_FASTOPEN_COOKIE_MIN &&
        len <= TCP_FASTOPEN_COOKIE_MAX)
        memcpy(foc->val, cookie, len);
    else if (len != 0)
        len = -1;
    foc->len = len;
    foc->exp = exp_opt;
}
\end{minted}
        \textcolor{blue}{调用位置}:

            %\begin{enumerate}
                %\item[1]       \ref{SYN+ACK:tcp_rcv_synsent_state_process}的tcp\_rcv\_synsent\_state\_process中调用。
            %\end{enumerate}
        \subsubsection{\mintinline{C}{tcp_parse_options}}
            \label{TCPOptions:tcp_parse_options}
                关于TCP Options的格式，如果不清楚的话，可以参考\ref{RFC793TCPHeader:tcp_header_format}中的介绍。
\begin{minted}[linenos]{C}
/* 
Location:

    net/ipv4/tcp_input.c

Function:

    Look for tcp options. Normally only called on SYN and SYNACK packets.
    But, this can also be called on packets in the established flow when
    the fast version below fails.

Parameter:

    skb:缓存块
    opt_rx:TCP的选项部分
    eatab:标志是否建立。
    foc:表明是否是快速打开。
*/
void tcp_parse_options(const struct sk_buff *skb,
               struct tcp_options_received *opt_rx, int estab,
               struct tcp_fastopen_cookie *foc)
{
const unsigned char *ptr;
const struct tcphdr *th = tcp_hdr(skb);
/*选项部分长度*/
int length = (th->doff * 4) - sizeof(struct tcphdr);
/*得到选项的首部*/
ptr = (const unsigned char *)(th + 1);
/*标记没有看到时间戳*/  
opt_rx->saw_tstamp = 0;

while (length > 0) {
    /*选项的类型*/
    int opcode = *ptr++;
    /*选项长度的大小*/      
    int opsize;

    switch (opcode) {
    //End of options
    case TCPOPT_EOL:
        return;
    //Padding           
    case TCPOPT_NOP:    /* Ref: RFC 793 section 3.1 */
        length--;
        continue;
    default:
        /*
            得到选项的长度，选项长度不可能小于2,
            具体参见上面的宏定义说明。
        */
        opsize = *ptr++;
        if (opsize < 2)     /* "silly options" */
            return;
        /*也不可能大于length*/
        if (opsize > length)
            return; /* don't parse partial options */
        switch (opcode) {
            /*最大报文段长度,只用于最初的协商阶段*/
            case TCPOPT_MSS:
                if (opsize == TCPOLEN_MSS && th->syn && !estab) {
                    /*取最大段长度,如果为0,那怎么办呢？这里似乎并没有处理*/
                    u16 in_mss = get_unaligned_be16(ptr);
                    if (in_mss) {
                        /*
                            将最大段长度设置为选项中和用户自身的较小值。
                        */
                        if (opt_rx->user_mss &&
                            opt_rx->user_mss < in_mss)
                            in_mss = opt_rx->user_mss;
                        opt_rx->mss_clamp = in_mss;
                    }
                }
                break;
            /*窗口扩大因子选项，也只能出现在SYN且未建立的情况下，并且支持该选项*/
            case TCPOPT_WINDOW:
                if (opsize == TCPOLEN_WINDOW && th->syn &&
                    !estab && sysctl_tcp_window_scaling) {
                    /*取位移数*/
                    __u8 snd_wscale = *(__u8 *)ptr;
                    /*将表示包含窗口扩大因子的选项置位*/
                    opt_rx->wscale_ok = 1;
                    /*大于14则警告，并设重新设置为14.*/
                    if (snd_wscale > 14) {
                        net_info_ratelimited("%s: Illegal window scaling value %d >14 received\n",
                                     __func__,
                                     snd_wscale);
                        snd_wscale = 14;
                    }
                    opt_rx->snd_wscale = snd_wscale;
                }
                break;
            /*时间戳选项，满足条件为：在建立的时候或者未建立但支持时间戳选项*/
            case TCPOPT_TIMESTAMP:
                if ((opsize == TCPOLEN_TIMESTAMP) &&
                    ((estab && opt_rx->tstamp_ok) ||
                     (!estab && sysctl_tcp_timestamps))) {
                    /*看见时间戳*/                          
                    opt_rx->saw_tstamp = 1;
                    /*time stamp value*/
                    opt_rx->rcv_tsval = get_unaligned_be32(ptr);
                    /*time stamp echo reply*/
                    opt_rx->rcv_tsecr = get_unaligned_be32(ptr + 4);
                }
                break;
            /*SACK Permit，只能在syn并且未建立的时候使用*/
            case TCPOPT_SACK_PERM:
                if (opsize == TCPOLEN_SACK_PERM && th->syn &&
                    !estab && sysctl_tcp_sack) {
                    //标记看见
                    opt_rx->sack_ok = TCP_SACK_SEEN;
                    /*
                        将与SACK有关的字段清零dsack,eff_sacks,num_sacks
                    */
                    tcp_sack_reset(opt_rx);
                }
                break;
            /*表明有SACK*/
            case TCPOPT_SACK:
                /*
                    TCPOLEN_SACK_BASE为2,TCPOLEN_SACK_PERBLOCK为8
                    因此，if语句的意思是，该选项包含kind、length以及
                    一个左右边界值对，且扣除kind和length字段之后，
                    nnegbei左右边界值对的大小整除，sack_ok为1表示
                    允许SACK.
                */
                if ((opsize >= (TCPOLEN_SACK_BASE + TCPOLEN_SACK_PERBLOCK)) &&
                   !((opsize - TCPOLEN_SACK_BASE) % TCPOLEN_SACK_PERBLOCK) &&
                   opt_rx->sack_ok) {
                    /*将sacked的值指向这些左右边界值对的起始处*/
                    TCP_SKB_CB(skb)->sacked = (ptr - 2) - (unsigned char *)th;
                }
                break;
#ifdef CONFIG_TCP_MD5SIG
            case TCPOPT_MD5SIG:
                /*
                 * The MD5 Hash has already been
                 * checked (see tcp_v{4,6}_do_rcv()).
                 */
                break;
#endif
            /*快打开*/
            case TCPOPT_FASTOPEN:
                tcp_parse_fastopen_option(
                    opsize - TCPOLEN_FASTOPEN_BASE,
                    ptr, th->syn, foc, false);
                break;
            /*实验阶段的Fast Open，现在基本上已经成形了。*/
            case TCPOPT_EXP:
                /* Fast Open option shares code 254 using a
                 * 16 bits magic number.
                 */
                if (opsize >= TCPOLEN_EXP_FASTOPEN_BASE &&
                    get_unaligned_be16(ptr) ==
                    TCPOPT_FASTOPEN_MAGIC)
                    tcp_parse_fastopen_option(opsize -
                        TCPOLEN_EXP_FASTOPEN_BASE,
                        ptr + 2, th->syn, foc, true);
                break;

        }
        /*进入下一个循环，处理下一个选项*/
        ptr += opsize-2;
        length -= opsize;
}
}
}
\end{minted}
        \textcolor{blue}{调用位置}:

            \begin{enumerate}
                \item[1]        \ref{SYN+ACK:tcp_rcv_synsent_state_process}的tcp\_rcv\_synsent\_state\_process中调用。
            \end{enumerate}
        \subsubsection{\mintinline{C}{tcp_clear_options}}
            \label{TCPOptions:tcp_clear_options}
\begin{minted}[linenos]{C}
/*
Location:

    include/linux/tcp.h
Function:
    
    主要是使时间戳标记，SACK，窗口扩大选项设置为0。
*/
static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
{
    rx_opt->tstamp_ok = rx_opt->sack_ok = 0;
    rx_opt->wscale_ok = rx_opt->snd_wscale = 0;
}
\end{minted}

            \begin{enumerate}
                \item[1]        \label{Server:tcp_conn_request}的tcp\_conn\_request中调用。
            \end{enumerate}
\subsubsection{\mintinline{c}{tcp_syn_option}}
该函数用户构建带SYN选项的TCP包。当然，这里只是计算出TCP的选项字段需要多大空间，
并不是真正地将选项构建成了TCP标准中所规定的格式。
\begin{minted}[linenos]{c}
/* Location: net/ipv4/tcp_output.c
 *
 * Parameter:
 *     sk: 套接字
 *     skb: 数据报
 *     opts: 用于存放TCP选项的指针
 *     md5: MD5码（如果开启了相关选项）
 */
static unsigned int tcp_syn_options(struct sock *sk, struct sk_buff *skb,
                                struct tcp_out_options *opts,
                                struct tcp_md5sig_key **md5)
{
        struct tcp_sock *tp = tcp_sk(sk);
        unsigned int remaining = MAX_TCP_OPTION_SPACE;
        struct tcp_fastopen_request *fastopen = tp->fastopen_req;

        /* remaining代表剩余了多少空间。根据RFC793，TCP选项的空间是有限的。
         * 在后面的代码中，每增加一个选项就会相应地减少remaining。
         */
#ifdef CONFIG_TCP_MD5SIG
        *md5 = tp->af_specific->md5_lookup(sk, sk);
        if (*md5) {
                opts->options |= OPTION_MD5;
                remaining -= TCPOLEN_MD5SIG_ALIGNED;
        }
#else
        *md5 = NULL;
#endif

        /* We always get an MSS option.  The option bytes which will be seen in
         * normal data packets should timestamps be used, must be in the MSS
         * advertised.  But we subtract them from tp->mss_cache so that
         * calculations in tcp_sendmsg are simpler etc.  So account for this
         * fact here if necessary.  If we don't do this correctly, as a
         * receiver we won't recognize data packets as being full sized when we
         * should, and thus we won't abide by the delayed ACK rules correctly.
         * SACKs don't matter, we never delay an ACK when we have any of those
         * going out.  */
        opts->mss = tcp_advertise_mss(sk);
        remaining -= TCPOLEN_MSS_ALIGNED;

        if (likely(sysctl_tcp_timestamps && !*md5)) {
                opts->options |= OPTION_TS;
                opts->tsval = tcp_skb_timestamp(skb) + tp->tsoffset;
                opts->tsecr = tp->rx_opt.ts_recent;
                remaining -= TCPOLEN_TSTAMP_ALIGNED;
        }
        if (likely(sysctl_tcp_window_scaling)) {
                opts->ws = tp->rx_opt.rcv_wscale;
                opts->options |= OPTION_WSCALE;
                remaining -= TCPOLEN_WSCALE_ALIGNED;
        }
        if (likely(sysctl_tcp_sack)) {
                opts->options |= OPTION_SACK_ADVERTISE;
                if (unlikely(!(OPTION_TS & opts->options)))
                        remaining -= TCPOLEN_SACKPERM_ALIGNED;
        }

        if (fastopen && fastopen->cookie.len >= 0) {
                u32 need = fastopen->cookie.len;

                need += fastopen->cookie.exp ? TCPOLEN_EXP_FASTOPEN_BASE :
                                               TCPOLEN_FASTOPEN_BASE;
                need = (need + 3) & ~3U;  /* Align to 32 bits */
                /* 到这里会产生一个有趣的现象，remaining有可能不够用。
                 * 可以看到，如果选项中的剩余空间不够用了，那么就放弃启用Fast Open。
                 */
                if (remaining >= need) {
                        opts->options |= OPTION_FAST_OPEN_COOKIE;
                        opts->fastopen_cookie = &fastopen->cookie;
                        remaining -= need;
                        tp->syn_fastopen = 1;
                        tp->syn_fastopen_exp = fastopen->cookie.exp ? 1 : 0;
                }
        }

        return MAX_TCP_OPTION_SPACE - remaining;
}
\end{minted}

\subsubsection{\mintinline{c}{tcp_established_options}}
这个函数用于计算连接已经建立的状态下，TCP包的头部选项所占用的空间。通过和
\mintinline{c}{tcp_syn_options}函数的对比我们可以直观地看到，大部分TCP选项
都是在发起连接的时候放在TCP包里的。真正开始传输后，TCP包所需携带的选项信息
就少了很多。
\begin{minted}[linenos]{c}
static unsigned int tcp_established_options(struct sock *sk, struct sk_buff *skb,
                                        struct tcp_out_options *opts,
                                        struct tcp_md5sig_key **md5)
{       
        struct tcp_sock *tp = tcp_sk(sk);
        unsigned int size = 0;
        unsigned int eff_sacks;
        
        opts->options = 0;

#ifdef CONFIG_TCP_MD5SIG
        *md5 = tp->af_specific->md5_lookup(sk, sk);
        if (unlikely(*md5)) { 
                opts->options |= OPTION_MD5;
                size += TCPOLEN_MD5SIG_ALIGNED;
        }
#else   
        *md5 = NULL;
#endif
        
        if (likely(tp->rx_opt.tstamp_ok)) {
                opts->options |= OPTION_TS;
                opts->tsval = skb ? tcp_skb_timestamp(skb) + tp->tsoffset : 0;
                opts->tsecr = tp->rx_opt.ts_recent;
                size += TCPOLEN_TSTAMP_ALIGNED;
        }
        
        eff_sacks = tp->rx_opt.num_sacks + tp->rx_opt.dsack;
        if (unlikely(eff_sacks)) { 
                const unsigned int remaining = MAX_TCP_OPTION_SPACE - size;
                opts->num_sack_blocks =
                        min_t(unsigned int, eff_sacks,
                              (remaining - TCPOLEN_SACK_BASE_ALIGNED) /
                              TCPOLEN_SACK_PERBLOCK);
                size += TCPOLEN_SACK_BASE_ALIGNED +
                        opts->num_sack_blocks * TCPOLEN_SACK_PERBLOCK;
        }
        
        return size;
}
\end{minted}

        \textcolor{blue}{调用位置}:
            \begin{enumerate}
                \item[1]        \label{Server:tcp_conn_request}的tcp\_conn\_request中调用。
            \end{enumerate}

\section{TCP PAWS}
    \subsection{TCP PAWS Flags}
\begin{minted}[linenos]{C}
/*
Location

    include/net/tcp.h
*/
#define TCP_PAWS_24DAYS (60 * 60 * 24 * 24)
#define TCP_PAWS_MSL    60      /* Per-host timestamps are invalidated
                                * after this time. It should be equal
                                * (or greater than) TCP_TIMEWAIT_LEN
                                * to provide reliability equal to one
                                * provided by timewait state.
                                */
#define TCP_PAWS_WINDOW 1       /* Replay window for per-host
                                 * timestamps. It must be less than
                                 * minimal timewait lifetime.
                                */
\end{minted}

    \subsection{\mintinline{C}{tcp_paws_check}}

\begin{minted}[linenos]{C}
/*
Location:

    include/net/tcp.h

Function:

    序号绕回检测。

Parameter:
    
    rx_opt:接收到的tcp选项。
    paws_win:
*/
static inline bool tcp_paws_check(const struct tcp_options_received *rx_opt,
                  int paws_win)
{
    if ((s32)(rx_opt->ts_recent - rx_opt->rcv_tsval) <= paws_win)
        return true;
    /*判断从存储ts_recent到现在是否有24天的时间,有的话，就可能失效*/
    if (unlikely(get_seconds() >= rx_opt->ts_recent_stamp + TCP_PAWS_24DAYS))
        return true;
    /*
     * Some OSes send SYN and SYNACK messages with tsval=0 tsecr=0,
     * then following tcp messages have valid values. Ignore 0 value,
     * or else 'negative' tsval might forbid us to accept their packets.
     */
    if (!rx_opt->ts_recent)
        return true;
    return false;
}
\end{minted}


\section{TCP TimeStamp}
    \subsection{\mintinline{C}{tcp_store_ts_recent}}
\begin{minted}[linenos]{C}
/*
Location

    net/ipv4/tcp_input.c

Function:

    替换时间戳ts_recent

Parameter:

    tp:TCP sock
*/
static void tcp_store_ts_recent(struct tcp_sock *tp)
{
    /*替换时间戳为刚刚接收到的时间戳值*/
    tp->rx_opt.ts_recent = tp->rx_opt.rcv_tsval;
    /*记录下替换时间戳的时间*/
    tp->rx_opt.ts_recent_stamp = get_seconds();
}
\end{minted}
    \subsection{\mintinline{C}{tcp_replace_ts_recent}}
        \label{TCPTimestamp:tcp_replace_ts_recent}
\begin{minted}[linenos]{C}
/*
Location

    net/ipv4/tcp_input.c

Function:

    考虑是否要替换时间戳ts_recent

Parameter:

    tp:TCP sock
    seq:

*/
static void tcp_replace_ts_recent(struct tcp_sock *tp, u32 seq)
{
    if (tp->rx_opt.saw_tstamp && !after(seq, tp->rcv_wup)) {
        /* PAWS bug workaround wrt. ACK frames, the PAWS discard
         * extra check below makes sure this can only happen
         * for pure ACK frames.  -DaveM
         *
         * Not only, also it occurs for expired timestamps.
         */
        /*PAWS检测成功*/
        if (tcp_paws_check(&tp->rx_opt, 0))
            tcp_store_ts_recent(tp);
    }
}
\end{minted}

        \textcolor{blue}{调用位置}:
            \begin{enumerate}
                \item[1]        \label{ClientReceiveSYN+ACK:tcp_ack}的tcp\_ack中调用。
            \end{enumerate}

\section{TCP ACK}
    ACK机制与数据传输紧密关联可以体现在以下几个方面。
\begin{enumerate}
\item[1]    通过ACK可以使得发送方可以很容易地计算出数据往返时间。
\item[2]    因为ACK段中携带接收方的通告窗口，因此，此时接收方能
            接收的数据上限为通告的接收窗口大小，接收到ACK后，在正常
            情况下会引发下一个段的发送。
\item[3]    通过拥塞窗口的调节，TCP可以进行限制性地传输，以免网络
            拥塞。从接收ACK可以判断和评估当前网络的状况，从而进一步调整
            拥塞窗口。
\end{enumerate}
    \subsection{ACK Check}
        \subsubsection{\mintinline{C}{tcp_is_sack & tcp_is_reno & tcp_is_fack}}
\begin{minted}[linenos]{C}
/* 
Location:

    include/net/tcp.h

Description:
    These functions determine how the current flow behaves in respect of SACK
    handling. SACK is negotiated with the peer, and therefore it can vary
    between different flows.

    tcp_is_sack - SACK enabled
    tcp_is_reno - No SACK
    tcp_is_fack - FACK enabled, implies SACK enabled
*/
static inline int tcp_is_sack(const struct tcp_sock *tp)
{
    return tp->rx_opt.sack_ok;
}

static inline bool tcp_is_reno(const struct tcp_sock *tp)
{
    return !tcp_is_sack(tp);
}

static inline bool tcp_is_fack(const struct tcp_sock *tp)
{
    return tp->rx_opt.sack_ok & TCP_FACK_ENABLED;
}
\end{minted}
        \subsubsection{\mintinline{c}{tcp_ack_is_dubious}}
            \label{ACKCheck:tcp_ack_is_dubious}
\begin{minted}[linenos]{C}
/*
Location:

    net/ipv4/tcp_input.c

Function:

    判断一个ACK是否可疑。

Parameter:

    sk:传输控制块
    flag:FLAG标志位
*/
static inline bool tcp_ack_is_dubious(const struct sock *sk, const int flag)
{
    return !(flag & FLAG_NOT_DUP) || (flag & FLAG_CA_ALERT) ||
        inet_csk(sk)->icsk_ca_state != TCP_CA_Open;
}
\end{minted}

    什么样的ACK算是可疑的呢?如下:
\begin{enumerate}
\item[非FLAG\_NOT\_DUP] 通过查看相关的宏定义我们知道FLAG\_NOT\_DUP为(FLAG\_DATA|FLAG\_WIN\_UPDATE|FLAG\_ACKED)
                        而FLAG\_ACKED又为(FLAG\_DATA\_ACKED|FLAG\_SYN\_ACKED),故而
                        即其所包含的种类有接收的ACK段是(1)负荷数据携带的;(2)更新窗口的;(3)确认新数据的;(4)确认SYN段的。
                        如果上述四种都不属于那是可疑的了。
            
\item[FLAG\_CA\_ALERT]  通过查看相关的宏定义我们知道FLAG\_CA\_ALERT为(FLAG\_DATA\_SACKED|FLAG\_ECE)，如果被发现时确认新数据的ACK或者
                        在ACK中存在ECE标志，即收到显式拥塞通知，也认为是可疑的。\color{red}{似乎和上面的有矛盾，}
\item[非Open]           即当前拥塞状态不为Open。
\end{enumerate}

        \textcolor{blue}{调用位置}:
            \begin{enumerate}
                \item[1]        \label{ClientReceiveSYN+ACK:tcp_ack}的tcp\_ack中调用。
            \end{enumerate}

        \subsubsection{\mintinline{C}{tcp_check_sack_reneging}}
            如果接收到的确认ACK指向之前记录的SACK，这说明之前记录的SACK并没有反映接收方的真实状态。
            接收路径上很有可能已经有拥塞发生或者接收主机正在经历严重的拥塞甚至处理出现了BUG，
            因为按照正常的逻辑流程，接收的ACK不应该指向已记录的SACK，
            而应该指向SACK后面未接收的地方。通常情况下，此时接收方已经删除了保存到失序队列中的段。
            
            为了避免短暂奇怪的看起来像是违约的SACK导致更大量的重传，我们给接收者一些时间,即$max(RTT/2, 10ms)$
            以便于让他可以给我们更多的ACK，从而可以使得SACK的记分板变得正常一点。如果这个表面上的违约一直持续到重传时间结束，
            我们就把SACK的记分板清除掉。

\begin{minted}[linenos]{C}
/*
Location:

    net/ipv4/tcp_input.c

Function:

    If ACK arrived pointing to a remembered SACK, it means that our
    remembered SACKs do not reflect real state of receiver i.e.
    receiver _host_ is heavily congested (or buggy).

    To avoid big spurious(假的) retransmission bursts(爆发) due to transient SACK
    scoreboard oddities that look like reneging, we give the receiver a
    little time (max(RTT/2, 10ms)) to send us some more ACKs that will
    restore sanity to the SACK scoreboard. If the apparent reneging
    persists until this RTO then we'll clear the SACK scoreboard.

Paramater:

    sk:传输控制块。
    flag:相关标志位
*/
static bool tcp_check_sack_reneging(struct sock *sk, int flag)
{
    //如果确认接收方违约了。
    if (flag & FLAG_SACK_RENEGING) {
        struct tcp_sock *tp = tcp_sk(sk);
        //计算超时重传时间
        unsigned long delay = max(usecs_to_jiffies(tp->srtt_us >> 4),
                      msecs_to_jiffies(10));
        //更新超时重传定时器        ???有时间好好分析一下。
        inet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,
                      delay, TCP_RTO_MAX);
        return true;
    }
    return false;
}
\end{minted}
        \subsubsection{\mintinline{C}{tcp_limit_reno_sacked}}
\begin{minted}[linenos]{C}
/* 
Location:

    net/ipv4/tcp_input.c

Function:

    Limits sacked_out so that sum with lost_out isn't ever larger than
    packets_out. Returns false if sacked_out adjustement wasn't necessary.

Parameter:

    tp:???
*/
static bool tcp_limit_reno_sacked(struct tcp_sock *tp)
{
    u32 holes;//记录丢失的包，???很奇怪，为什么没能直接利用lost_out呢？
    //既然由重复的ACK，那么必然是之前传输的段有丢失的了，所以至少为1。
    holes = max(tp->lost_out, 1U);
    //但是又不能大于所有的发出去的。
    holes = min(holes, tp->packets_out);

    if ((tp->sacked_out + holes) > tp->packets_out) {
        tp->sacked_out = tp->packets_out - holes;
        return true;
    }
    return false;
}

     该函数用于检查sacked\_out是否过多，过多则限制，且返回true，说明出现reordering了。

    那么我们怎么判断是否有reordering呢？ 我们知道dupack可能由lost引起，也有可能由reorder引起，那么如果 
    sacked\_out + lost\_out > packets\_out，则说明sacked\_out偏大了，因为它错误的把由reorder 
    引起的dupack当客户端的sack了。
\end{minted}
        \subsubsection{\mintinline{C}{tcp_check_reno_reordering}}
\begin{minted}[linenos]{C}
/* 
Location:
    
    net/ipv4/tcp_input.c

Function:

    If we receive more dupacks than we expected counting segments
    in assumption of absent reordering, interpret this as reordering.
    The only another reason could be bug in receiver TCP.

Parameter:

    sk:传输控制块
    addend:???
*/
static void tcp_check_reno_reordering(struct sock *sk, const int addend)
{
    struct tcp_sock *tp = tcp_sk(sk);
    //判断是否有reordering了
    if (tcp_limit_reno_sacked(tp))
        tcp_update_reordering(sk, tp->packets_out + addend, 0);//???这个函数到底时用于干什么呢？
}
\end{minted}
    \subsection{ACK Update}
        \subsubsection{\mintinline{C}{tcp_snd_una_update}}
            \label{ACKUpdate:tcp_snd_una_update}
\begin{minted}[linenos]{C}
/* 
Location:

    net/ipv4/tcp_input.c

Description:

    If we update tp->snd_una, also update tp->bytes_acked */

Parameter:
    
    tp:tcp sock
    ack:确认号
static void tcp_snd_una_update(struct tcp_sock *tp, u32 ack)
{
    /*得到已经确认的数量*/
    u32 delta = ack - tp->snd_una;

    /*记录已经得到确认的字节数量*/
    u64_stats_update_begin(&tp->syncp);
    tp->bytes_acked += delta;
    u64_stats_update_end(&tp->syncp);
    /*更新una*/
    tp->snd_una = ack;
}
\end{minted}

        \textcolor{blue}{调用位置}:
            \begin{enumerate}
                \item[1]        \label{ClientReceiveSYN+ACK:tcp_ack}的tcp\_ack中调用。
            \end{enumerate}

        \subsubsection{\mintinline{C}{tcp_reset_reno_sack}}
\begin{minted}[linenos]{C}
/*
Location:

    net/ipv4/tcp_input.c

Function:

    将sacked_out字段置为0

Parameter:

    tp:???

*/
static inline void tcp_reset_reno_sack(struct tcp_sock *tp)
{
    tp->sacked_out = 0;
}
\end{minted}    

        \subsubsection{\mintinline{C}{tcp_add_reno_sack}}
\begin{minted}[linenos]{C}
/* 
Location:

    net/ipv4/tcp_input.c

Location:

    Emulate(仿真) SACKs for SACKless connection: account for a new dupack.

Parameter:

    sk:传输控制块
*/

static void tcp_add_reno_sack(struct sock *sk)
{
    struct tcp_sock *tp = tcp_sk(sk);
    tp->sacked_out++;       //增加sacked的包数.
    tcp_check_reno_reordering(sk, 0);// 检查是否有reordering
    tcp_verify_left_out(tp);    //判断left_out是否大于packets_out,出警告信息
}
\end{minted}
\section{TCP Window}
    \subsection{Window Compute}
        \subsubsection{\mintinline{C}{tcp_left_out}}
            该函数用于计算已经发出去的TCP段(离开主机)中一共有多少个tcp段还未得到确认。
\begin{minted}[linenos]{C}
static inline unsigned int tcp_left_out(const struct tcp_sock *tp)
{
    return tp->sacked_out + tp->lost_out;
}
\end{minted}
        \subsubsection{\mintinline{C}{tcp_verify_left_out}}
            该函数的功能主要是判断left\_out是否大于packets\_out,当然，这是不可能的，因为
            前者是已经发送离开主机的未被确认的段数，而后者是已经离开发送队列(不一定离开主机)
            但未确认的段数。故而，这里有一个WARN\_ON，以便输出相应的警告信息。
\begin{minted}[linenos]{C}
/* 
Location:

    include/net/tcp.h

Function:

    Use define here intentionally(有意地) to get WARN_ON location shown at the caller 

Parameter:

*/
#define tcp_verify_left_out(tp) WARN_ON(tcp_left_out(tp) > tp->packets_out)     
\end{minted}

    \subsection{Window Update}
        \subsubsection{\mintinline{C}{tcp_may_update_window}}
            \label{WindowUpdate:tcp_may_update_window}
\begin{minted}[linenos]{C}
/* 
Location:

    net/ipv4/tcp_input.c

Function:

    Check that window update is acceptable.
    The function assumes that snd_una<=ack<=snd_next.

Parameter:

    tp:tcp sock
    ack:起始序列号
    ack_seq:确认序列号
    nwin:窗口大小
*/
static inline bool tcp_may_update_window(const struct tcp_sock *tp,
                    const u32 ack, const u32 ack_seq,
                    const u32 nwin)
{
    /*
        三种情况成立一种
            1.起始序列号大于una
            2.确认号大于窗口的左端
            3.确认号等于窗口的左端，同时，窗口大于本身的发送窗口
    */
    return  after(ack, tp->snd_una) ||
        after(ack_seq, tp->snd_wl1) ||
        (ack_seq == tp->snd_wl1 && nwin > tp->snd_wnd);
}
\end{minted}
        \subsubsection{\mintinline{C}{tcp_ack_update_window}}
            \label{WindowUpdate:tcp_ack_update_window}
\begin{minted}[linenos]{C}
/* 
Location:

    net/ipv4/tcp_input.c

Description:

    Update our send window.

    Window update algorithm, described in RFC793/RFC1122 (used in linux-2.2
    and in FreeBSD. NetBSD's one is even worse.) is wrong.

Parameter:

    sk:传输控制块
    skb:缓存块
    ack:    
    ack_seq:
*/
static int tcp_ack_update_window(struct sock *sk, const struct sk_buff *skb, u32 ack,
                 u32 ack_seq)
{
    struct tcp_sock *tp = tcp_sk(sk);
    int flag = 0;
    /*得到接收窗口大小*/
    u32 nwin = ntohs(tcp_hdr(skb)->window);
    /*判断是否有syn，没有的话，窗口扩大*/
    if (likely(!tcp_hdr(skb)->syn))
        nwin <<= tp->rx_opt.snd_wscale;

    if (tcp_may_update_window(tp, ack, ack_seq, nwin)) {
        /*表明要更新*/
        flag |= FLAG_WIN_UPDATE;
        /*tp->snd_wl1 = seq*/
        tcp_update_wl(tp, ack_seq);
        /*更新本端发送窗口*/
        if (tp->snd_wnd != nwin) {
            tp->snd_wnd = nwin;

            /* Note, it is the only place, where
             * fast path is recovered for sending TCP.
             */
            /*取消头部预测*/
            tp->pred_flags = 0;
            /*快速路径检测*/
            tcp_fast_path_check(sk);
            /*??*/
            if (tcp_send_head(sk))
                tcp_slow_start_after_idle_check(sk);
            /*如果接收窗口大于之前的最大的接收窗口，更新，重新计算MSS*/
            if (nwin > tp->max_window) {
                tp->max_window = nwin;
                tcp_sync_mss(sk, inet_csk(sk)->icsk_pmtu_cookie);
            }
        }
    }
    /*更新una*/
    tcp_snd_una_update(tp, ack);

    return flag;
}
\end{minted}

        \textcolor{blue}{调用位置}:
            \begin{enumerate}
                \item[1]        \label{ClientReceiveSYN+ACK:tcp_ack}的tcp\_ack中调用。
            \end{enumerate}

\section{TCP Urgent Data}
        \subsubsection{相应标识}

\begin{minted}[linenos]{C}
/*
Location:

    include/net/tcp.h
*/
#define TCP_URG_VALID   0x0100
#define TCP_URG_NOTYET  0x0200
#define TCP_URG_READ    0x0400
\end{minted}
        \subsubsection{\mintinline{C}{tcp_check_urg}}
            \label{TCPUrgent:tcp_check_urg}

\begin{minted}[linenos]{C}
/*
Location:

    net/ipv4/tcp_input.c

Function:

    This routine is only called when we have urgent data
    signaled. Its the 'slow' part of tcp_urg. It could be
    moved inline now as tcp_urg is only called from one
    place. We handle URGent data wrong. We have to - as
    BSD still doesn't use the correction from RFC961.
    For 1003.1g we should support a new option TCP_STDURG to permit
    either form (or just set the sysctl tcp_stdurg).

Parameter:

    sk:传输控制块
    th:tcp头部
*/

static void tcp_check_urg(struct sock *sk, const struct tcphdr *th)
{
    struct tcp_sock *tp = tcp_sk(sk);
    /*字节序转换*/
    u32 ptr = ntohs(th->urg_ptr);
    /*
        计算带外数据的位置
    注意:
        目前仍有许多关于紧急指针是指向带外数据的最后一个字节还是指向
        带外数据最后一个字节的下一个字节。Host Requirements RFC确定
        指向最后一个字节是正确的。然而，大多数的实现，包括BSD的实现，
        继续使用错误的方法来实现。因此如果启用兼容错误选项，就需要将
        指针前移。
    */
    if (ptr && !sysctl_tcp_stdurg)
        ptr--;
    ptr += ntohl(th->seq);

    /* Ignore urgent data that we've already seen and read. 
        如果我们已经读取过或者接收过相应的数据，就忽略。
    */
    if (after(tp->copied_seq, ptr))
        return;

    /* Do not replay urg ptr.
     *
     * NOTE: interesting situation not covered by specs.
     * Misbehaving sender may send urg ptr, pointing to segment,
     * which we already have in ofo queue. We are not able to fetch
     * such data and will stay in TCP_URG_NOTYET until will be eaten
     * by recvmsg(). Seems, we are not obliged to handle such wicked
     * situations. But it is worth to think about possibility of some
     * DoSes using some hypothetical application level deadlock.
     */
    if (before(ptr, tp->rcv_nxt))
        return;

    /* Do we already have a newer (or duplicate) urgent pointer? 
        如果还有带外数据用户进程尚未读取，并且当前带外数据早于该
        尚未读取的带外数据，则丢弃该带外数据。
    */
    if (tp->urg_data && !after(ptr, tp->urg_seq))
        return;

    /* Tell the world about our new urgent pointer. 
        唤醒异步等待该套接口的进程，并告诉它们有新的带外数据到达。
    */
    sk_send_sigurg(sk);

    /* We may be adding urgent data when the last byte read was
     * urgent. To do this requires some care. We cannot just ignore
     * tp->copied_seq since we would read the last urgent byte again
     * as data, nor can we alter copied_seq until this data arrives
     * or we break the semantics of SIOCATMARK (and thus sockatmark())
     *
     * NOTE. Double Dutch. Rendering to plain English: author of comment
     * above did something sort of  send("A", MSG_OOB); send("B", MSG_OOB);
     * and expect that both A and B disappear from stream. This is _wrong_.
     * Though this happens in BSD with high probability, this is occasional.
     * Any application relying on this is buggy. Note also, that fix "works"
     * only in this artificial test. Insert some normal data between A and B and we will
     * decline of BSD again. Verdict: it is better to remove to trap
     * buggy users.
     */
    /*
        在当前接收到的带外数据的段的序号正是接下来需要复制到进程空间
        的段的序号，且下一个接收段的序号与需要复制到进程空间的序号相等
        的情况下，需要检测接收队列中的第一个段是否有效。因为当前处理的
        是带外数据，因此需要递增copied_seq。接着检测接收队列中第一个段
        是否已经国企，如果是，则释放。
    */
    if (tp->urg_seq == tp->copied_seq && tp->urg_data &&
        !sock_flag(sk, SOCK_URGINLINE) && tp->copied_seq != tp->rcv_nxt) {
        struct sk_buff *skb = skb_peek(&sk->sk_receive_queue);
        tp->copied_seq++;
        if (skb && !before(tp->copied_seq, TCP_SKB_CB(skb)->end_seq)) {
            __skb_unlink(skb, &sk->sk_receive_queue);
            __kfree_skb(skb);
        }
    }
    /*
        设置带外数据序号以及标记，以便后续根据标志作相应处理。
    */
    tp->urg_data = TCP_URG_NOTYET;
    tp->urg_seq = ptr;

    /* Disable header prediction. 
        由于接收到了带外数据，禁止下次首部预测。    
    */
    tp->pred_flags = 0;
}
\end{minted}
        \subsubsection{\mintinline{C}{tcp_urg}}
            \label{TCPUrgent:tcp_urg}
\begin{minted}[linenos]{C}
/*
Location:

    net/ipv4/tcp_input.c

Function:

    This is the 'fast' part of urgent handling. 

Parameter:

    sk:传输控制块
    skb:缓存块
    th:TCP首部
*/
static void tcp_urg(struct sock *sk, struct sk_buff *skb, const struct tcphdr *th)
{
    struct tcp_sock *tp = tcp_sk(sk);

    /* Check if we get a new urgent pointer - normally not. 
        检查带外数据偏移量是否正常。
    */
    if (th->urg)
        tcp_check_urg(sk, th);

    /* Do we wait for any urgent data? - normally not... */
    if (tp->urg_data == TCP_URG_NOTYET) {
        u32 ptr = tp->urg_seq - ntohl(th->seq) + (th->doff * 4) -
              th->syn;

        /* Is the urgent pointer pointing into this packet? */
        if (ptr < skb->len) {
            u8 tmp;
            if (skb_copy_bits(skb, ptr, &tmp, 1))
                BUG();
            /*
                设置TCP_URG_VALID标志表示用户进程可以读取，然后通知
                用户进程读取。
            */
            tp->urg_data = TCP_URG_VALID | tmp;
            if (!sock_flag(sk, SOCK_DEAD))
                sk->sk_data_ready(sk);
        }
    }
}
\end{minted}    

        \textcolor{blue}{调用位置}:

            \begin{enumerate}
            \item[1]        \ref{SYN+ACK:tcp_rcv_state_process}的tcp\_rcv\_state\_process中调用。
            \end{enumerate}

    \subsection{\mintinline{c}{tcp_fin_time}}
        计算等待接收FIN的超时时间。超时时间至少为$\frac{7}{2}$倍的rto。
\begin{minted}[linenos]{c}
static inline int tcp_fin_time(const struct sock *sk)
{
        int fin_timeout = tcp_sk(sk)->linger2 ? : sysctl_tcp_fin_timeout;
        const int rto = inet_csk(sk)->icsk_rto;

        if (fin_timeout < (rto << 2) - (rto >> 1))
                fin_timeout = (rto << 2) - (rto >> 1);

        return fin_timeout;
}
\end{minted}
    \subsection{About Congestion Control}
        \subsubsection{\mintinline{C}{tcp_end_cwnd_reduction}}
\begin{minted}[linenos]{C}
/* 
Location:

    net/ipv4/tcp_input.c

Function:

    结束拥塞窗口减小。

Parameter:

    sk:传输控制块。remain to do in the future ????
*/
static inline void tcp_end_cwnd_reduction(struct sock *sk)
{
    struct tcp_sock *tp = tcp_sk(sk);

    /* Reset cwnd to ssthresh in CWR or Recovery (unless it's undone) */
    if (inet_csk(sk)->icsk_ca_state == TCP_CA_CWR ||
        (tp->undo_marker && tp->snd_ssthresh < TCP_INFINITE_SSTHRESH)) {
        tp->snd_cwnd = tp->snd_ssthresh;
        tp->snd_cwnd_stamp = tcp_time_stamp;
    }
    tcp_ca_event(sk, CA_EVENT_COMPLETE_CWR);
}
\end{minted}
        \subsubsection{\mintinline{c}{tcp_try_undo_recovery}}
\begin{minted}[linenos]{C}
/* 
Location:

    net/ipv4/tcp_input.c

Function:

    尝试从恢复状态撤销。

Parameter:

    sk:传输控制块。remain to do in the future ????
*/
static bool tcp_try_undo_recovery(struct sock *sk)
{
    struct tcp_sock *tp = tcp_sk(sk);

    if (tcp_may_undo(tp)) {
        int mib_idx;

        /* Happy end! We did not retransmit anything
         * or our original transmission succeeded.
         */
        DBGUNDO(sk, inet_csk(sk)->icsk_ca_state == TCP_CA_Loss ? "loss" : "retrans");
        tcp_undo_cwnd_reduction(sk, false);
        if (inet_csk(sk)->icsk_ca_state == TCP_CA_Loss)
            mib_idx = LINUX_MIB_TCPLOSSUNDO;
        else
            mib_idx = LINUX_MIB_TCPFULLUNDO;

        NET_INC_STATS_BH(sock_net(sk), mib_idx);
    }
    if (tp->snd_una == tp->high_seq && tcp_is_reno(tp)) {
        /* Hold old state until something *above* high_seq
         * is ACKed. For Reno it is MUST to prevent false
         * fast retransmits (RFC2582). SACK TCP is safe. */
        tcp_moderate_cwnd(tp);
        if (!tcp_any_retrans_done(sk))
            tp->retrans_stamp = 0;
        return true;
    }
    tcp_set_ca_state(sk, TCP_CA_Open);
    return false;
}
\end{minted}
    \subsection{About Congestion Control}
        \subsubsection{\mintinline{C}{sysctl_tcp_recovery}}
\begin{minted}[linenos]{C}
int sysctl_tcp_recovery __read_mostly = TCP_RACK_LOST_RETRANS;
\end{minted}
        \subsubsection{\mintinline{C}{tcp_rack_mark_lost}}
\begin{minted}[linenos]{C}
/*
Location:

    net/ipv4/tcp_recovery.c

Function:

    Marks a packet lost, if some packet sent later has been (s)acked.
    The underlying idea is similar to the traditional dupthresh and FACK
    but they look at different metrics:

    dupthresh: 3 OOO packets delivered (packet count)
    FACK: sequence delta to highest sacked sequence (sequence space)
    RACK: sent time delta to the latest delivered packet (time domain)

    The advantage of RACK is it applies to both original and retransmitted
    packet and therefore is robust against tail losses. Another advantage
    is being more resilient to reordering by simply allowing some
    "settling delay", instead of tweaking the dupthresh.

    The current version is only used after recovery starts but can be
    easily extended to detect the first loss.

Parameter:



*/

int tcp_rack_mark_lost(struct sock *sk)
{
    struct tcp_sock *tp = tcp_sk(sk);
    struct sk_buff *skb;
    u32 reo_wnd, prior_retrans = tp->retrans_out;

    if (inet_csk(sk)->icsk_ca_state < TCP_CA_Recovery || !tp->rack.advanced)
        return 0;

    /* Reset the advanced flag to avoid unnecessary queue scanning */
    tp->rack.advanced = 0;

    /* To be more reordering resilient, allow min_rtt/4 settling delay
     * (lower-bounded to 1000uS). We use min_rtt instead of the smoothed
     * RTT because reordering is often a path property and less related
     * to queuing or delayed ACKs.
     *
     * TODO: measure and adapt to the observed reordering delay, and
     * use a timer to retransmit like the delayed early retransmit.
     */
    reo_wnd = 1000;
    if (tp->rack.reord && tcp_min_rtt(tp) != ~0U)
        reo_wnd = max(tcp_min_rtt(tp) >> 2, reo_wnd);

    tcp_for_write_queue(skb, sk) {
        struct tcp_skb_cb *scb = TCP_SKB_CB(skb);

        if (skb == tcp_send_head(sk))
            break;

        /* Skip ones already (s)acked */
        if (!after(scb->end_seq, tp->snd_una) ||
            scb->sacked & TCPCB_SACKED_ACKED)
            continue;

        if (skb_mstamp_after(&tp->rack.mstamp, &skb->skb_mstamp)) {

            if (skb_mstamp_us_delta(&tp->rack.mstamp,
                        &skb->skb_mstamp) <= reo_wnd)
                continue;

            /* skb is lost if packet sent later is sacked */
            tcp_skb_mark_lost_uncond_verify(tp, skb);
            if (scb->sacked & TCPCB_SACKED_RETRANS) {
                scb->sacked &= ~TCPCB_SACKED_RETRANS;
                tp->retrans_out -= tcp_skb_pcount(skb);
                NET_INC_STATS_BH(sock_net(sk),
                         LINUX_MIB_TCPLOSTRETRANSMIT);
            }
        } else if (!(scb->sacked & TCPCB_RETRANS)) {
            /* Original data are sent sequentially so stop early
             * b/c the rest are all sent after rack_sent
             */
            break;
        }
    }
    return prior_retrans - tp->retrans_out;
}

/* Record the most recently (re)sent time among the (s)acked packets */
void tcp_rack_advance(struct tcp_sock *tp,
              const struct skb_mstamp *xmit_time, u8 sacked)
{
    if (tp->rack.mstamp.v64 &&
        !skb_mstamp_after(xmit_time, &tp->rack.mstamp))
        return;

    if (sacked & TCPCB_RETRANS) {
        struct skb_mstamp now;

        /* If the sacked packet was retransmitted, it's ambiguous
         * whether the retransmission or the original (or the prior
         * retransmission) was sacked.
         *
         * If the original is lost, there is no ambiguity. Otherwise
         * we assume the original can be delayed up to aRTT + min_rtt.
         * the aRTT term is bounded by the fast recovery or timeout,
         * so it's at least one RTT (i.e., retransmission is at least
         * an RTT later).
         */
        skb_mstamp_get(&now);
        if (skb_mstamp_us_delta(&now, xmit_time) < tcp_min_rtt(tp))
            return;
    }

    tp->rack.mstamp = *xmit_time;
    tp->rack.advanced = 1;
}
\end{minted}
        \subsubsection{\mintinline{C}{}}
\begin{minted}[linenos]{C}
/*
Location:

    net/ipv4/tcp_input.c

Function:

    Try to undo cwnd reduction, because D-SACKs acked all retransmitted data

Parameter:

    sk:传输控制块
*/
static bool tcp_try_undo_dsack(struct sock *sk)
{
    struct tcp_sock *tp = tcp_sk(sk);

    if (tp->undo_marker && !tp->undo_retrans) {
        DBGUNDO(sk, "D-SACK");
        tcp_undo_cwnd_reduction(sk, false);
        NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPDSACKUNDO);
        return true;
    }
    return false;
}
\end{minted}
        \subsubsection{\mintinline{C}{tcp_process_loss}}
\begin{minted}[linenos]{C}
/* 
Location:

    net/ipv4/tcp_input.c

Function:

    Process an ACK in CA_Loss state. Move to CA_Open if lost data are
    recovered or spurious. Otherwise retransmits more on partial ACKs.

Parameter:

    sk:传输控制块
    flag:
    is_dupack:
*/
static void tcp_process_loss(struct sock *sk, int flag, bool is_dupack)
{
    struct tcp_sock *tp = tcp_sk(sk);
    bool recovered = !before(tp->snd_una, tp->high_seq);

    if ((flag & FLAG_SND_UNA_ADVANCED) &&
        tcp_try_undo_loss(sk, false))
        return;

    if (tp->frto) { /* F-RTO RFC5682 sec 3.1 (sack enhanced version). */
        /* Step 3.b. A timeout is spurious if not all data are
         * lost, i.e., never-retransmitted data are (s)acked.
         */
        if ((flag & FLAG_ORIG_SACK_ACKED) &&
            tcp_try_undo_loss(sk, true))
            return;

        if (after(tp->snd_nxt, tp->high_seq)) {
            if (flag & FLAG_DATA_SACKED || is_dupack)
                tp->frto = 0; /* Step 3.a. loss was real */
        } else if (flag & FLAG_SND_UNA_ADVANCED && !recovered) {
            tp->high_seq = tp->snd_nxt;
            __tcp_push_pending_frames(sk, tcp_current_mss(sk),
                          TCP_NAGLE_OFF);
            if (after(tp->snd_nxt, tp->high_seq))
                return; /* Step 2.b */
            tp->frto = 0;
        }
    }

    if (recovered) {
        /* F-RTO RFC5682 sec 3.1 step 2.a and 1st part of step 3.a */
        tcp_try_undo_recovery(sk);
        return;
    }
    if (tcp_is_reno(tp)) {
        /* A Reno DUPACK means new data in F-RTO step 2.b above are
         * delivered. Lower inflight to clock out (re)tranmissions.
         */
        if (after(tp->snd_nxt, tp->high_seq) && is_dupack)
            tcp_add_reno_sack(sk);
        else if (flag & FLAG_SND_UNA_ADVANCED)
            tcp_reset_reno_sack(tp);
    }
    tcp_xmit_retransmit_queue(sk);
}
\end{minted}
        \subsubsection{\mintinline{C}{tcp_try_undo_partial}}
\begin{minted}[linenos]{C}
/*
Location:

    net/ipv4/tcp_input.c

Function:

    Undo during fast recovery after partial ACK. 

Parameter:

    sk:传输控制块。
    acked:
    prior_unacked:
    flag:
*/
static bool tcp_try_undo_partial(struct sock *sk, const int acked,
                 const int prior_unsacked, int flag)
{
    struct tcp_sock *tp = tcp_sk(sk);

    if (tp->undo_marker && tcp_packet_delayed(tp)) {
        /* Plain luck! Hole if filled with delayed
         * packet, rather than with a retransmit.
         */
        tcp_update_reordering(sk, tcp_fackets_out(tp) + acked, 1);

        /* We are getting evidence that the reordering degree is higher
         * than we realized. If there are no retransmits out then we
         * can undo. Otherwise we clock out new packets but do not
         * mark more packets lost or retransmit more.
         */
        if (tp->retrans_out) {
            tcp_cwnd_reduction(sk, prior_unsacked, 0, flag);
            return true;
        }

        if (!tcp_any_retrans_done(sk))
            tp->retrans_stamp = 0;

        DBGUNDO(sk, "partial recovery");
        tcp_undo_cwnd_reduction(sk, true);
        NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPPARTIALUNDO);
        tcp_try_keep_open(sk);
        return true;
    }
    return false;
}
\end{minted}
    \subsection{About Restransmit}
        \subsubsection{\mintinline{C}{ Related Macro}}
\begin{minted}[linenos]{C}
/*
    include/net/tcp.h
*/
/* Use TCP RACK to detect (some) tail and retransmit losses */
#define TCP_RACK_LOST_RETRANS  0x1
\end{minted}

\subsection{\mintinline{c}{tcp_done}}
\begin{minted}[linenos]{c}
/* 该函数用于完成关闭TCP连接，回收并清理相关资源。 */
void tcp_done(struct sock *sk)
{
        struct request_sock *req = tcp_sk(sk)->fastopen_rsk;

        /* 当套接字状态为SYN_SENT或SYN_RECV时，更新统计数据。 */
        if (sk->sk_state == TCP_SYN_SENT || sk->sk_state == TCP_SYN_RECV)
                TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_ATTEMPTFAILS);

        /* 将连接状态设置为关闭，并清除定时器。 */
        tcp_set_state(sk, TCP_CLOSE);
        tcp_clear_xmit_timers(sk);
        /* 当启用了Fast Open时，移除fastopen请求 */
        if (req)
                reqsk_fastopen_remove(sk, req, false);

        sk->sk_shutdown = SHUTDOWN_MASK;

        /* 如果状态不为SOCK_DEAD，则唤醒等待着的进程。 */
        if (!sock_flag(sk, SOCK_DEAD))
                sk->sk_state_change(sk);
        else
                inet_csk_destroy_sock(sk);
}
\end{minted}



    \subsection{TCP Close}
        \subsubsection{TCP Close Flags}
            这是关于是接收到关闭连接还是说发送了关闭连接的标志。
\begin{minted}[linenos]{C}
/*
Location
    include/net/sock.h
*/
#define SHUTDOWN_MASK   3
#define RCV_SHUTDOWN    1
#define SEND_SHUTDOWN   2
\end{mimted}
        \subsubsection{TCP State}
\begin{minted}[linenos]{C}
/*include/net/tcp_state.c*/
enum {
    TCP_ESTABLISHED = 1,
    TCP_SYN_SENT,
    TCP_SYN_RECV,
    TCP_FIN_WAIT1,
    TCP_FIN_WAIT2,
    TCP_TIME_WAIT,
    TCP_CLOSE,
    TCP_CLOSE_WAIT,
    TCP_LAST_ACK,
    TCP_LISTEN,
    TCP_CLOSING,    /* Now a valid state */
    TCP_NEW_SYN_RECV,

    TCP_MAX_STATES  /* Leave at the end! */
};

#define TCP_STATE_MASK  0xF
\end{minted}

        \subsubsection{TCP State Machine}

\begin{minted}[linenos]{c}
/*include/net/tcp_state.c*/
#define TCP_ACTION_FIN  (1 << 7)
/*Location:net/ipv4/tcp.c*/
static const unsigned char new_state[16] = {
  /* 当前状态:             新的状态:        动作:      */
  [0 /* (Invalid) */]   = TCP_CLOSE,
  [TCP_ESTABLISHED]     = TCP_FIN_WAIT1 | TCP_ACTION_FIN,
  [TCP_SYN_SENT]        = TCP_CLOSE,
  [TCP_SYN_RECV]        = TCP_FIN_WAIT1 | TCP_ACTION_FIN,
  [TCP_FIN_WAIT1]       = TCP_FIN_WAIT1,
  [TCP_FIN_WAIT2]       = TCP_FIN_WAIT2,
  [TCP_TIME_WAIT]       = TCP_CLOSE,
  [TCP_CLOSE]           = TCP_CLOSE,
  [TCP_CLOSE_WAIT]      = TCP_LAST_ACK  | TCP_ACTION_FIN,
  [TCP_LAST_ACK]        = TCP_LAST_ACK,
  [TCP_LISTEN]          = TCP_CLOSE,
  [TCP_CLOSING]         = TCP_CLOSING,
  [TCP_NEW_SYN_RECV]    = TCP_CLOSE,    /* should not happen ! */
};
\end{minted}
    \subsection{\mintinline{C}{tcp_shutdown}}

        \mintinline{C}{tcp_shutdown}是TCP的shutdown系统调用的传输层接口实现，由套接口层的实现\mintinline{C}{inet_shutdown}调用。
\begin{minted}[linenos]{C}
/*
Location:

    net/ipv4/tcp.c

Function:
    
    Shutdown the sending side of a connection. Much like close except
    that we don't receive shut down or sock_set_flag(sk, SOCK_DEAD).

Parameter:

    sk:传输控制块。
    how:
*/
void tcp_shutdown(struct sock *sk, int how)
{
    /*  We need to grab some memory, and put together a FIN,
     *  and then put it into the queue to be sent.
     *      Tim MacKenzie(tym@dibbler.cs.monash.edu.au) 4 Dec '92.
     */
    if (!(how & SEND_SHUTDOWN))
        return;

    /* If we've already sent a FIN, or it's a closed state, skip this. */
    if ((1 << sk->sk_state) &
        (TCPF_ESTABLISHED | TCPF_SYN_SENT |
         TCPF_SYN_RECV | TCPF_CLOSE_WAIT)) {
        /* Clear out any half completed packets.  FIN if needed. */
        if (tcp_close_state(sk))
            tcp_send_fin(sk);
    }
}
\end{minted}
        如果是发送方向的关闭，并且TCP状态为ESTABLISHED、SYN\_SENT、SYN\_RECV或CLOSE\_WAIT时，根据TC状态迁移图和当前的状态设置新的状态，并在需要发送FIN时，调用FIN时，调用\mintinline{C}{tcp_send_fin}时向对方发送FIN。

        而对于接收方向的关闭，则无需向对方发送FIN，因为可能还需要向对方发送数据。至于接收方向的关闭的实现，在recvmsg系统调用中发现设置了RCV\_SHUTDOWN标志会立即返回。


