\chapter{非核心函数分析}

\minitoc

\section{SKB}

\section{Inet}
    \subsection{\mintinline{C}{inet_hash_connect && __inet_hash_connect}}

        \subsubsection{\mintinline{C}{inet_hash_connect}}
\begin{minted}[linenos]{C}
/*
 * Bind a port for a connect operation and hash it.
 */
int inet_hash_connect(struct inet_timewait_death_row *death_row,
              struct sock *sk)
{
    u32 port_offset = 0;

    if (!inet_sk(sk)->inet_num)
        port_offset = inet_sk_port_offset(sk);
    return __inet_hash_connect(death_row, sk, port_offset,
                   __inet_check_established);
}
\end{minted}

        \subsubsection{\mintinline{C}{__inet_hash_connect}}     
\begin{minted}[linenos]{C}
int __inet_hash_connect(struct inet_timewait_death_row *death_row,
        struct sock *sk, u32 port_offset,
        int (*check_established)(struct inet_timewait_death_row *,
            struct sock *, __u16, struct inet_timewait_sock **))
{
    struct inet_hashinfo *hinfo = death_row->hashinfo;
    const unsigned short snum = inet_sk(sk)->inet_num;
    struct inet_bind_hashbucket *head;
    struct inet_bind_bucket *tb;
    int ret;
    struct net *net = sock_net(sk);

    if (!snum) {
        int i, remaining, low, high, port;
        static u32 hint;
        u32 offset = hint + port_offset;
        struct inet_timewait_sock *tw = NULL;

        inet_get_local_port_range(net, &low, &high);
        remaining = (high - low) + 1;

        /* By starting with offset being an even number,
         * we tend to leave about 50% of ports for other uses,
         * like bind(0).
         */
        offset &= ~1;

        local_bh_disable();
        for (i = 0; i < remaining; i++) {
            port = low + (i + offset) % remaining;
            if (inet_is_local_reserved_port(net, port))
                continue;
            head = &hinfo->bhash[inet_bhashfn(net, port,
                    hinfo->bhash_size)];
            spin_lock(&head->lock);

            /* Does not bother with rcv_saddr checks,
             * because the established check is already
             * unique enough.
             */
            inet_bind_bucket_for_each(tb, &head->chain) {
                if (net_eq(ib_net(tb), net) &&
                    tb->port == port) {
                    if (tb->fastreuse >= 0 ||
                        tb->fastreuseport >= 0)
                        goto next_port;
                    WARN_ON(hlist_empty(&tb->owners));
                    if (!check_established(death_row, sk,
                                port, &tw))
                        goto ok;
                    goto next_port;
                }
            }

            tb = inet_bind_bucket_create(hinfo->bind_bucket_cachep,
                    net, head, port);
            if (!tb) {
                spin_unlock(&head->lock);
                break;
            }
            tb->fastreuse = -1;
            tb->fastreuseport = -1;
            goto ok;

        next_port:
            spin_unlock(&head->lock);
        }
        local_bh_enable();

        return -EADDRNOTAVAIL;

ok:
        hint += (i + 2) & ~1;

        /* Head lock still held and bh's disabled */
        inet_bind_hash(sk, tb, port);
        if (sk_unhashed(sk)) {
            inet_sk(sk)->inet_sport = htons(port);
            inet_ehash_nolisten(sk, (struct sock *)tw);
        }
        if (tw)
            inet_twsk_bind_unhash(tw, hinfo);
        spin_unlock(&head->lock);

        if (tw)
            inet_twsk_deschedule_put(tw);

        ret = 0;
        goto out;
    }

    head = &hinfo->bhash[inet_bhashfn(net, snum, hinfo->bhash_size)];
    tb  = inet_csk(sk)->icsk_bind_hash;
    spin_lock_bh(&head->lock);
    if (sk_head(&tb->owners) == sk && !sk->sk_bind_node.next) {
        inet_ehash_nolisten(sk, NULL);
        spin_unlock_bh(&head->lock);
        return 0;
    } else {
        spin_unlock(&head->lock);
        /* No definite answer... Walk to established hash table */
        ret = check_established(death_row, sk, snum, NULL);
out:
        local_bh_enable();
        return ret;
    }
}
\end{minted}

\subsection{\mintinline{c}{inet_twsk_put}}
该函数用于释放\mintinline{c}{inet_timewait_sock}结构体。
\begin{minted}[linenos]{c}
void inet_twsk_put(struct inet_timewait_sock *tw)
{
        /* 减小引用计数，如果计数为0，则释放它 */
        if (atomic_dec_and_test(&tw->tw_refcnt))
                inet_twsk_free(tw);
}
\end{minted}

\section{TCP层}

    \mintinline{c}{TCP_INC_STATS_BH}

    \mintinline{c}{rcu_read_unlock} 出现在\mintinline{c}{tcp_make_synack}中于MD5相关的部分。
    
    \mintinline{c}{net_xmit_eval}   定时器？？
	\subsection{TCP相关参数}
\begin{enumerate}
\item[tcp\_syn\_retries]  INTEGER,
默认值是5
对于一个新建连接，内核要发送多少个 SYN 连接请求才决定放弃。不应该大于255，默认值是5，对应于180秒左右时间。(对于大负载而物理通信良好的网络而言,这个值偏高,可修改为2.这个值仅仅是针对对外的连接,对进来的连接,是由
tcp\_retries1 决定的)

\item[tcp\_synack\_retries] INTEGER,
默认值是5
对于远端的连接请求SYN，内核会发送SYN ＋ ACK数据报，以确认收到上一个 SYN连接请求包。这是所谓的三次握手( threeway handshake)机制的第二个步骤。这里决定内核在放弃连接之前所送出的 SYN+ACK 数目。不应该大于255，默认值是5，对应于180秒左右时间。(可以根据上面的tcp\_syn\_retries来决定这个值)

\item[tcp\_keepalive\_time] INTEGER,
默认值是7200(2小时)
当keepalive打开的情况下，TCP发送keepalive消息的频率。(由于目前网络攻击等因素,造成了利用这个进行的攻击很频繁,曾经也有cu的朋友提到过,说如果2边建立了连接,然后不发送任何数据或者rst/fin消息,那么持续的时间是不是就是2小时,空连接攻击?tcp\_keepalive\_time就是预防此情形的.我个人在做nat服务的时候的修改值为1800秒)

\item[tcp\_keepalive\_probes] INTEGER,
默认值是9
TCP发送keepalive探测以确定该连接已经断开的次数。(注意:保持连接仅在SO\_KEEPALIVE套接字选项被打开是才发送.次数默认不需要修改,当然根据情形也可以适当地缩短此值.设置为5比较合适)

\item[tcp\_keepalive\_intvl] INTEGER,
默认值为75
探测消息发送的频率，乘以tcp\_keepalive\_probes就得到对于从开始探测以来没有响应的连接杀除的时间。默认值为75秒，也就是没有活动的连接将在大约11分钟以后将被丢弃。(对于普通应用来说,这个值有一些偏大,可以根据需要改小.特别是web类服务器需要改小该值,15是个比较合适的值)

\item[tcp\_retries1] INTEGER,
默认值是3
放弃回应一个TCP连接请求前﹐需要进行多少次重试。RFC 规定最低的数值是3﹐这也是默认值﹐根据RTO的值大约在3秒 - 8分钟之间。(注意:这个值同时还决定进入的syn连接)

\item[tcp\_retries2] INTEGER,
默认值为15
在丢弃激活(已建立通讯状况)的TCP连接之前﹐需要进行多少次重试。默认值为15，根据RTO的值来决定，相当于13-30分钟(RFC1122规定，必须大于100秒).(这个值根据目前的网络设置,可以适当地改小,我的网络内修改为了5)

\item[tcp\_orphan\_retries] INTEGER,
默认值是7
在近端丢弃TCP连接之前﹐要进行多少次重试。默认值是7个﹐相当于 50秒 - 16分钟﹐视 RTO 而定。如果您的系统是负载很大的web服务器﹐那么也许需要降低该值﹐这类 sockets 可能会耗费大量的资源。另外参的考tcp\_max\_orphans 。(事实上做NAT的时候,降低该值也是好处显著的,我本人的网络环境中降低该值为3)

\item[tcp\_fin\_timeout] INTEGER,
默认值是 60
对于本端断开的socket连接，TCP保持在FIN-WAIT-2状态的时间。对方可能会断开连接或一直不结束连接或不可预料的进程死亡。默认值为 60 秒。过去在2.2版本的内核中是 180 秒。您可以设置该值﹐但需要注意﹐如果您的机器为负载很重的web服务器﹐您可能要冒内存被大量无效数据报填满的风险﹐FIN-WAIT-2 sockets 的危险性低于 FIN-WAIT-1 ﹐因为它们最多只吃 1.5K 的内存﹐但是它们存在时间更长。另外参考 tcp\_max\_orphans。(事实上做NAT的时候,降低该值也是好处显著的,我本人的网络环境中降低该值为30)

\item[tcp\_max\_tw\_buckets] INTEGER,
默认值是180000
系统在同时所处理的最大 timewait sockets 数目。如果超过此数的话﹐time-wait socket 会被立即砍除并且显示警告信息。之所以要设定这个限制﹐纯粹为了抵御那些简单的 DoS 攻击﹐千万不要人为的降低这个限制﹐不过﹐如果网络条件需要比默认值更多﹐则可以提高它(或许还要增加内存)。(事实上做NAT的时候最好可以适当地增加该值)

\item[tcp\_tw\_recycle] BOOLEAN,
默认值是0
打开快速 TIME-WAIT sockets 回收。除非得到技术专家的建议或要求﹐请不要随意修改这个值。(做NAT的时候，建议打开它)

\item[tcp\_tw\_reuse] BOOLEAN,
默认值是0
该文件表示是否允许重新应用处于TIME-WAIT状态的socket用于新的TCP连接(这个对快速重启动某些服务,而启动后提示端口已经被使用的情形非常有帮助)

\item[tcp\_max\_orphans] INTEGER,
缺省值是8192
系统所能处理不属于任何进程的TCP sockets最大数量。假如超过这个数量﹐那么不属于任何进程的连接会被立即reset，并同时显示警告信息。之所以要设定这个限制﹐纯粹为了抵御那些简单的 DoS 攻击﹐千万不要依赖这个或是人为的降低这个限制(这个值Redhat AS版本中设置为32768,但是很多防火墙修改的时候,建议该值修改为2000)

\item[tcp\_abort\_on\_overflow] BOOLEAN,
缺省值是0
当守护进程太忙而不能接受新的连接，就象对方发送reset消息，默认值是false。这意味着当溢出的原因是因为一个偶然的猝发，那么连接将恢复状态。只有在你确信守护进程真的不能完成连接请求时才打开该选项，该选项会影响客户的使用。(对待已经满载的sendmail,apache这类服务的时候,这个可以很快让客户端终止连接,可以给予服务程序处理已有连接的缓冲机会,所以很多防火墙上推荐打开它)

\item[tcp\_syncookies] BOOLEAN,
默认值是0
只有在内核编译时选择了CONFIG\_SYNCOOKIES时才会发生作用。当出现syn等候队列出现溢出时象对方发送syncookies。目的是为了防止syn flood攻击。
注意：该选项千万不能用于那些没有收到攻击的高负载服务器，如果在日志中出现synflood消息，但是调查发现没有收到synflood攻击，而是合法用户的连接负载过高的原因，你应该调整其它参数来提高服务器性能。参考:
	tcp\_max\_syn\_backlog、
	tcp\_synack\_retries、
	tcp\_abort\_on\_overflow。
syncookie严重的违背TCP协议，不允许使用TCP扩展，可能对某些服务导致严重的性能影响(如SMTP转发)。(注意,该实现与BSD上面使用的tcp proxy一样,是违反了RFC中关于tcp连接的三次握手实现的,但是对于防御syn-flood的确很有用.)

\item[tcp\_stdurg] BOOLEAN,
默认值为0
使用 TCP urg pointer 字段中的主机请求解释功能。大部份的主机都使用老旧的 BSD解释，因此如果您在 Linux 打开它﹐或会导致不能和它们正确沟通。

\item[tcp\_max\_syn\_backlog] INTEGER,
对于那些依然还未获得客户端确认的连接请求﹐需要保存在队列中最大数目。对于超过 128Mb 内存的系统﹐默认值是1024 ﹐低于 128Mb 的则为 128。如果服务器经常出现过载﹐可以尝试增加这个数字。警告﹗假如您将此值设为大于1024﹐最好修改 include/net/tcp.h 里面的 TCP\_SYNQ\_HSIZE ﹐以保持TCP\_SYNQ\_HSIZE*16<=tcp\_max\_syn\_backlog ﹐并且编进核心之内。(SYN Flood攻击利用TCP协议散布握手的缺陷，伪造虚假源IP地址发送大量TCP-SYN半打开连接到目标系统，最终导致目标系统Socket队列资源耗尽而无法接受新的连接。为了应付这种攻击，现代Unix系统中普遍采用多连接队列处理的方式来缓冲(而不是解决)这种攻击，是用一个基本队列处理正常的完全连接应用(Connect()和Accept() )，是用另一个队列单独存放半打开连接。这种双队列处理方式和其他一些系统内核措施(例如Syn-Cookies/Caches)联合应用时，能够比较有效的缓解小规模的SYN Flood攻击(事实证明<1000p/s)加大SYN队列长度可以容纳更多等待连接的网络连接数，所以对Server来说可以考虑增大该值.)

\item[tcp\_window\_scaling] INTEGER,
缺省值为1
该文件表示设置tcp/ip会话的滑动窗口大小是否可变。参数值为布尔值，为1时表示可变，为0时表示不可变。tcp/ip通常使用的窗口最大可达到 65535 字节，对于高速网络，该值可能太小，这时候如果启用了该功能，可以使tcp/ip滑动窗口大小增大数个数量级，从而提高数据传输的能力(RFC 1323)。（对普通地百M网络而言，关闭会降低开销，所以如果不是高速网络，可以考虑设置为0）

\item[tcp\_timestamps] BOOLEAN,
缺省值为1
Timestamps 用在其它一些东西中﹐可以防范那些伪造的 sequence 号码。一条1G的宽带线路或许会重遇到带 out-of-line数值的旧sequence 号码(假如它是由于上次产生的)。Timestamp 会让它知道这是个 '旧封包'。(该文件表示是否启用以一种比超时重发更精确的方法（RFC 1323）来启用对 RTT 的计算；为了实现更好的性能应该启用这个选项。)

\item[tcp\_sack] BOOLEAN,
缺省值为1
使用 Selective ACK﹐它可以用来查找特定的遗失的数据报--- 因此有助于快速恢复状态。该文件表示是否启用有选择的应答（Selective Acknowledgment），这可以通过有选择地应答乱序接收到的报文来提高性能（这样可以让发送者只发送丢失的报文段）。(对于广域网通信来说这个选项应该启用，但是这会增加对 CPU 的占用。)

\item[tcp\_fack] BOOLEAN,
缺省值为1
打开FACK拥塞避免和快速重传功能。(注意，当tcp\_sack设置为0的时候，这个值即使设置为1也无效)

\item[tcp\_dsack] BOOLEAN,
缺省值为1
允许TCP发送"两个完全相同"的SACK。

\item[tcp\_ecn] BOOLEAN,
缺省值为0
打开TCP的直接拥塞通告功能。

\item[tcp\_reordering] INTEGER,
默认值是3
TCP流中重排序的数据报最大数量 。 (一般有看到推荐把这个数值略微调整大一些,比如5)

\item[tcp\_retrans\_collapse] BOOLEAN，
缺省值为1
对于某些有bug的打印机提供针对其bug的兼容性。(一般不需要这个支持,可以关闭它)

\item[tcp\_wmem(3个INTEGER变量)] min, default, max。
min：为TCP socket预留用于发送缓冲的内存最小值。每个tcp socket都可以在建议以后都可以使用它。默认值为4096(4K)。

default：为TCP socket预留用于发送缓冲的内存数量，默认情况下该值会影响其它协议使用的net.core.wmem\_default 值，一般要低于net.core.wmem\_default的值。默认值为16384(16K)。

max: 用于TCP socket发送缓冲的内存最大值。该值不会影响net.core.wmem\_max，"静态"选择参数SO\_SNDBUF则不受该值影响。默认值为131072(128K)。（对于服务器而言，增加这个参数的值对于发送数据很有帮助,在我的网络环境中,修改为了51200 131072 204800）

\item[tcp\_rmem (3个INTEGER变量)] min, default, max。
min：为TCP socket预留用于接收缓冲的内存数量，即使在内存出现紧张情况下tcp socket都至少会有这么多数量的内存用于接收缓冲，默认值为8K。

default：为TCP socket预留用于接收缓冲的内存数量，默认情况下该值影响其它协议使用的net.core.wmem\_default 值。该值决定了在tcp\_adv\_win\_scale、tcp\_app\_win和tcp\_app\_win=0默认值情况下，TCP窗口大小为65535。默认值为87380

max：用于TCP socket接收缓冲的内存最大值。该值不会影响 net.core.wmem\_max，"静态"选择参数 SO\_SNDBUF则不受该值影响。默认值为 128K。默认值为87380*2 bytes。（可以看出，.max的设置最好是default的两倍,对于NAT来说主要该增加它,我的网络里为 51200 131072 204800）

\item[tcp\_mem(3个INTEGER变量)] low, pressure, high。
low：当TCP使用了低于该值的内存页面数时，TCP不会考虑释放内存。(理想情况下，这个值应与指定给 tcp\_wmem 的第 2 个值相匹配 - 这第 2 个值表明，最大页面大小乘以最大并发请求数除以页大小 (131072 * 300 / 4096)。 )

pressure：当TCP使用了超过该值的内存页面数量时，TCP试图稳定其内存使用，进入pressure模式，当内存消耗低于low值时则退出pressure状态。(理想情况下这个值应该是 TCP 可以使用的总缓冲区大小的最大值 (204800 * 300 / 4096)。 )

high：允许所有tcp sockets用于排队缓冲数据报的页面量。(如果超过这个值，TCP 连接将被拒绝，这就是为什么不要令其过于保守 (512000 * 300 / 4096) 的原因了。 在这种情况下，提供的价值很大，它能处理很多连接，是所预期的 2.5 倍；或者使现有连接能够传输 2.5 倍的数据。 我的网络里为192000 300000 732000)

一般情况下这些值是在系统启动时根据系统内存数量计算得到的。

\item[tcp\_app\_win] INTEGER,
默认值是31
保留max($window/2^{tcp\_app\_win}$, mss)数量的窗口由于应用缓冲。当为0时表示不需要缓冲。

\item[tcp\_adv\_win\_scale] INTEGER,
默认值为2
计算缓冲开销$bytes/2^{tcp\_adv\_win\_scale}$(如果tcp\_adv\_win\_scale > 0)或者$bytes-bytes/2^{-tcp\_adv\_win\_scale}$(如果tcp\_adv\_win\_scale <= 0）。

\item[tcp\_rfc1337] BOOLEAN,
缺省值为0
这个开关可以启动对于在RFC1337中描述的"tcp 的time-wait暗杀危机"问题的修复。启用后，内核将丢弃那些发往time-wait状态TCP套接字的RST 包.

\item[tcp\_low\_latency]  BOOLEAN,
缺省值为0
允许 TCP/IP 栈适应在高吞吐量情况下低延时的情况；这个选项一般情形是的禁用。(但在构建Beowulf 集群的时候,打开它很有帮助)

\item[tcp\_westwood] BOOLEAN,
缺省值为0
启用发送者端的拥塞控制算法，它可以维护对吞吐量的评估，并试图对带宽的整体利用情况进行优化；对于 WAN 通信来说应该启用这个选项。

\item[tcp\_bic] BOOLEAN,
缺省值为0
为快速长距离网络启用 Binary Increase Congestion；这样可以更好地利用以 GB 速度进行操作的链接；对于 WAN 通信应该启用这个选项。
\end{enumerate}
	\subsection{TCP相关宏定义}
		\subsubsection{标志宏}
\begin{minted}[linenos]{C}
#define FLAG_DATA		0x01 		/* Incoming frame contained data.		*/
#define FLAG_WIN_UPDATE		0x02 	/* Incoming ACK was a window update.	*/
#define FLAG_DATA_ACKED		0x04 	/* This ACK acknowledged new data.		*/
#define FLAG_RETRANS_DATA_ACKED	0x08 /* "" "" some of which was retransmitted.	*/
#define FLAG_SYN_ACKED		0x10 	/* This ACK acknowledged SYN.		*/
#define FLAG_DATA_SACKED	0x20 	/* New SACK.				*/
#define FLAG_ECE		0x40 		/* ECE in this ACK				*/
#define FLAG_LOST_RETRANS	0x80 /* This ACK marks some retransmission lost */
#define FLAG_SLOWPATH		0x100 /* Do not skip RFC checks for window update.*/
#define FLAG_ORIG_SACK_ACKED	0x200 /* Never retransmitted data are (s)acked	*/
#define FLAG_SND_UNA_ADVANCED	0x400 /* Snd_una was changed (!= FLAG_DATA_ACKED) */
#define FLAG_DSACKING_ACK	0x800 /* SACK blocks contained D-SACK info */
#define FLAG_SACK_RENEGING	0x2000 /* snd_una advanced to a sacked seq */
#define FLAG_UPDATE_TS_RECENT	0x4000 /* tcp_replace_ts_recent() */

#define FLAG_ACKED		(FLAG_DATA_ACKED|FLAG_SYN_ACKED)
#define FLAG_NOT_DUP		(FLAG_DATA|FLAG_WIN_UPDATE|FLAG_ACKED)
#define FLAG_CA_ALERT		(FLAG_DATA_SACKED|FLAG_ECE)
#define FLAG_FORWARD_PROGRESS	(FLAG_ACKED|FLAG_DATA_SACKED)

#define TCP_REMNANT (TCP_FLAG_FIN|TCP_FLAG_URG|TCP_FLAG_SYN|TCP_FLAG_PSH)
#define TCP_HP_BITS (~(TCP_RESERVED_BITS|TCP_FLAG_PSH))
\end{minted}

			这里我们需要介绍几个相关的概念，如下：

\begin{enumerate}
\item[SACK]			Selective Acknowledgment(SACK)，这种方式需要在TCP头里加一个SACK的东西，ACK还是Fast Retransmit的ACK，
					SACK则是汇报收到的数据碎版。这样，在发送端就可以根据回传的SACK来知道哪些数据到了，哪些没有到。
					于是就优化了 Fast Retransmit的算法。当然，这个协议需要两边都支持。
\item[Reneging]		所谓Reneging的意思就是违约，接收方有权把已经报给发送端SACK里的数据给丢了。
					当然，我们肯定是不鼓励这样做的，因为这个事会把问题复杂化。
					但是，接收方可能会由于一些极端情况这么做，比如要把内存给别的更重要的东西。
					所以，发送方也不能完全依赖SACK，主要还是要依赖ACK，并维护Time-Out。
					如果后续的ACK没有增长，那么还是要把SACK的东西重传，另外，接收端这边永远不能把SACK的包标记为Ack。
　　					\color{red}{注意}：SACK会消费发送方的资源，试想，如果一个攻击者给数据发送方发一堆SACK的选项，
					这会导致发送方开始要重传甚至遍历已经发出的数据，这会消耗很多发送端的资源。

\item[D-SACK]		Duplicate SACK又称D-SACK，其主要使用了SACK来告诉发送方有哪些数据被重复接收了。
					D-SACK使用了SACK的第一个段来做标志，如果SACK的第一个段的范围被ACK所覆盖，那么就是D-SACK。
					如果SACK的第一个段的范围被SACK的第二个段覆盖，那么就是D-SACK。引入了D-SACK，有这么几个好处：
　						1）可以让发送方知道，是发出去的包丢了，还是回来的ACK包丢了。

　　						2）是不是自己的timeout太小了，导致重传。

　						3）网络上出现了先发的包后到的情况（又称reordering）

　　						4）网络上是不是把我的数据包给复制了。
\end{enumerate}

	\subsection{\mintinline{c}{__tcp_push_pending_frames}}
\begin{minted}[linenos]{c}
/* 将等待在队列中的包全部发出。 */
void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
                               int nonagle)
{
        /* 如果此时连接已经关闭了，那么直接返回。*/
        if (unlikely(sk->sk_state == TCP_CLOSE))
                return;

        /* 关闭nagle算法，将剩余的部分发送出去。 */
        if (tcp_write_xmit(sk, cur_mss, nonagle, 0,
                           sk_gfp_atomic(sk, GFP_ATOMIC)))
                tcp_check_probe_timer(sk);
}
\end{minted}
	\subsection{About ACK}
		\subsubsection{\mintinline{C}{tcp_is_sack & tcp_is_reno & tcp_is_fack}}
\begin{minted}[linenos]{C}
/* 
Location:

	include/net/tcp.h

Description:
	These functions determine how the current flow behaves in respect of SACK
	handling. SACK is negotiated with the peer, and therefore it can vary
	between different flows.

	tcp_is_sack - SACK enabled
	tcp_is_reno - No SACK
	tcp_is_fack - FACK enabled, implies SACK enabled
*/
static inline int tcp_is_sack(const struct tcp_sock *tp)
{
	return tp->rx_opt.sack_ok;
}

static inline bool tcp_is_reno(const struct tcp_sock *tp)
{
	return !tcp_is_sack(tp);
}

static inline bool tcp_is_fack(const struct tcp_sock *tp)
{
	return tp->rx_opt.sack_ok & TCP_FACK_ENABLED;
}
\end{minted}

		\subsubsection{\mintinline{C}{tcp_reset_reno_sack}}
\begin{minted}[linenos]{C}
/*
Location:

	net/ipv4/tcp_input.c

Function:

	将sacked_out字段置为0

Parameter:

	tp:???

*/
static inline void tcp_reset_reno_sack(struct tcp_sock *tp)
{
	tp->sacked_out = 0;
}
\end{minted}	
		\subsubsection{\mintinline{c}{tcp_ack_is_dubious}}
\begin{minted}[linenos]{C}
/*
Location:

	net/ipv4/tcp_input.c

Function:

	判断一个ACK是否可疑。

Parameter:

	sk:传输控制块
	flag:FLAG标志位
*/
static inline bool tcp_ack_is_dubious(const struct sock *sk, const int flag)
{
	return !(flag & FLAG_NOT_DUP) || (flag & FLAG_CA_ALERT) ||
		inet_csk(sk)->icsk_ca_state != TCP_CA_Open;
}
\end{minted}

	什么样的ACK算是可疑的呢?如下:
\begin{enumerate}
\item[非FLAG\_NOT\_DUP]	通过查看相关的宏定义我们知道FLAG\_NOT\_DUP为(FLAG\_DATA|FLAG\_WIN\_UPDATE|FLAG\_ACKED)
						而FLAG\_ACKED又为(FLAG\_DATA\_ACKED|FLAG\_SYN\_ACKED),故而
						即其所包含的种类有接收的ACK段是(1)负荷数据携带的;(2)更新窗口的;(3)确认新数据的;(4)确认SYN段的。
						如果上述四种都不属于那是可疑的了。
			
\item[FLAG\_CA\_ALERT]	通过查看相关的宏定义我们知道FLAG\_CA\_ALERT为(FLAG\_DATA\_SACKED|FLAG\_ECE)，如果被发现时确认新数据的ACK或者
						在ACK中存在ECE标志，即收到显式拥塞通知，也认为是可疑的。\color{red}{似乎和上面的有矛盾，}
\item[非Open]			即当前拥塞状态不为Open。
\end{enumerate}
		\subsubsection{\mintinline{C}{tcp_check_sack_reneging}}
			如果接收到的确认ACK指向之前记录的SACK，这说明之前记录的SACK并没有反映接收方的真实状态。
			接收路径上很有可能已经有拥塞发生或者接收主机正在经历严重的拥塞甚至处理出现了BUG，
			因为按照正常的逻辑流程，接收的ACK不应该指向已记录的SACK，
			而应该指向SACK后面未接收的地方。通常情况下，此时接收方已经删除了保存到失序队列中的段。
			
			为了避免短暂奇怪的看起来像是违约的SACK导致更大量的重传，我们给接收者一些时间,即$max(RTT/2, 10ms)$
			以便于让他可以给我们更多的ACK，从而可以使得SACK的记分板变得正常一点。如果这个表面上的违约一直持续到重传时间结束，
			我们就把SACK的记分板清除掉。
\begin{minted}[linenos]{C}
/*
Location:

	net/ipv4/tcp_input.c

Function:

	If ACK arrived pointing to a remembered SACK, it means that our
	remembered SACKs do not reflect real state of receiver i.e.
	receiver _host_ is heavily congested (or buggy).

	To avoid big spurious(假的) retransmission bursts(爆发) due to transient SACK
	scoreboard oddities that look like reneging, we give the receiver a
	little time (max(RTT/2, 10ms)) to send us some more ACKs that will
	restore sanity to the SACK scoreboard. If the apparent reneging
	persists until this RTO then we'll clear the SACK scoreboard.

Paramater:

	sk:传输控制块。
	flag:相关标志位
*/
static bool tcp_check_sack_reneging(struct sock *sk, int flag)
{
	//如果确认接收方违约了。
	if (flag & FLAG_SACK_RENEGING) {
		struct tcp_sock *tp = tcp_sk(sk);
		//计算超时重传时间
		unsigned long delay = max(usecs_to_jiffies(tp->srtt_us >> 4),
					  msecs_to_jiffies(10));
		//更新超时重传定时器		???有时间好好分析一下。
		inet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,
					  delay, TCP_RTO_MAX);
		return true;
	}
	return false;
}
\end{minted}

	\subsection{About Window Size and Segment Sent}
		
		\subsubsection{\mintinline{C}{tcp_left_out}}
			该函数用于计算已经发出去的TCP段(离开主机)中一共有多少个tcp段还未得到确认。
\begin{minted}[linenos]{C}
static inline unsigned int tcp_left_out(const struct tcp_sock *tp)
{
	return tp->sacked_out + tp->lost_out;
}
\end{minted}
		\subsubsection{\mintinline{C}{tcp_verify_left_out}}
			该函数的功能主要是判断left\_out是否大于packets\_out,当然，这是不可能的，因为
			前者是已经发送离开主机的未被确认的段数，而后者是已经离开发送队列(不一定离开主机)
			但未确认的段数。故而，这里有一个WARN\_ON，以便输出相应的警告信息。
\begin{minted}[linenos]{C}
/* 
Location:

	include/net/tcp.h

Function:

	Use define here intentionally(有意地) to get WARN_ON location shown at the caller 

Parameter:

*/
#define tcp_verify_left_out(tp)	WARN_ON(tcp_left_out(tp) > tp->packets_out)		
\end{minted}
\subsection{\mintinline{c}{tcp_fin_time}}
计算等待接收FIN的超时时间。超时时间至少为$\frac{7}{2}$倍的rto。
\begin{minted}[linenos]{c}
static inline int tcp_fin_time(const struct sock *sk)
{
        int fin_timeout = tcp_sk(sk)->linger2 ? : sysctl_tcp_fin_timeout;
        const int rto = inet_csk(sk)->icsk_rto;

        if (fin_timeout < (rto << 2) - (rto >> 1))
                fin_timeout = (rto << 2) - (rto >> 1);

        return fin_timeout;
}
\end{minted}
	\subsection{About Congestion Control}
		\subsubsection{\mintinline{C}{tcp_end_cwnd_reduction}}
\begin{minted}[linenos]{C}
/* 
Location:

	net/ipv4/tcp_input.c

Function:

	结束拥塞窗口减小。

Parameter:

	sk:传输控制块。remain to do in the future ????
*/
static inline void tcp_end_cwnd_reduction(struct sock *sk)
{
	struct tcp_sock *tp = tcp_sk(sk);

	/* Reset cwnd to ssthresh in CWR or Recovery (unless it's undone) */
	if (inet_csk(sk)->icsk_ca_state == TCP_CA_CWR ||
	    (tp->undo_marker && tp->snd_ssthresh < TCP_INFINITE_SSTHRESH)) {
		tp->snd_cwnd = tp->snd_ssthresh;
		tp->snd_cwnd_stamp = tcp_time_stamp;
	}
	tcp_ca_event(sk, CA_EVENT_COMPLETE_CWR);
}
\end{minted}
		\subsubsection{\mintinline{c}{tcp_try_undo_recovery}}
\begin{minted}[linenos]{C}
/* 
Location:

	net/ipv4/tcp_input.c

Function:

	尝试从恢复状态撤销。

Parameter:

	sk:传输控制块。remain to do in the future ????
*/
static bool tcp_try_undo_recovery(struct sock *sk)
{
	struct tcp_sock *tp = tcp_sk(sk);

	if (tcp_may_undo(tp)) {
		int mib_idx;

		/* Happy end! We did not retransmit anything
		 * or our original transmission succeeded.
		 */
		DBGUNDO(sk, inet_csk(sk)->icsk_ca_state == TCP_CA_Loss ? "loss" : "retrans");
		tcp_undo_cwnd_reduction(sk, false);
		if (inet_csk(sk)->icsk_ca_state == TCP_CA_Loss)
			mib_idx = LINUX_MIB_TCPLOSSUNDO;
		else
			mib_idx = LINUX_MIB_TCPFULLUNDO;

		NET_INC_STATS_BH(sock_net(sk), mib_idx);
	}
	if (tp->snd_una == tp->high_seq && tcp_is_reno(tp)) {
		/* Hold old state until something *above* high_seq
		 * is ACKed. For Reno it is MUST to prevent false
		 * fast retransmits (RFC2582). SACK TCP is safe. */
		tcp_moderate_cwnd(tp);
		if (!tcp_any_retrans_done(sk))
			tp->retrans_stamp = 0;
		return true;
	}
	tcp_set_ca_state(sk, TCP_CA_Open);
	return false;
}
\end{minted}

\subsection{\mintinline{c}{tcp_done}}
\begin{minted}[linenos]{c}
/* 该函数用于完成关闭TCP连接，回收并清理相关资源。 */
void tcp_done(struct sock *sk)
{
        struct request_sock *req = tcp_sk(sk)->fastopen_rsk;

        /* 当套接字状态为SYN_SENT或SYN_RECV时，更新统计数据。 */
        if (sk->sk_state == TCP_SYN_SENT || sk->sk_state == TCP_SYN_RECV)
                TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_ATTEMPTFAILS);

        /* 将连接状态设置为关闭，并清除定时器。 */
        tcp_set_state(sk, TCP_CLOSE);
        tcp_clear_xmit_timers(sk);
        /* 当启用了Fast Open时，移除fastopen请求 */
        if (req)
                reqsk_fastopen_remove(sk, req, false);

        sk->sk_shutdown = SHUTDOWN_MASK;

        /* 如果状态不为SOCK_DEAD，则唤醒等待着的进程。 */
        if (!sock_flag(sk, SOCK_DEAD))
                sk->sk_state_change(sk);
        else
                inet_csk_destroy_sock(sk);
}
\end{minted}

\subsection{tcp\_init\_nondata\_skb}
该函数提供了的功能。函数如下：


\begin{minted}[linenos]{c}
Location:

	net/ipv4/tcp_output.c
	
Function:

	初始化不含数据的skb.

Parameter:

	skb:待初始化的sk_buff。
  	seq:序号
	flags:标志位
static void tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags)
{
        /* 设置校验码 */
        skb->ip_summed = CHECKSUM_PARTIAL;
        skb->csum = 0;

        /* 设置标志位 */
        TCP_SKB_CB(skb)->tcp_flags = flags;
        TCP_SKB_CB(skb)->sacked = 0;

        tcp_skb_pcount_set(skb, 1);

        /* 设置起始序号 */
        TCP_SKB_CB(skb)->seq = seq;
        if (flags & (TCPHDR_SYN | TCPHDR_FIN))
                seq++;
        TCP_SKB_CB(skb)->end_seq = seq;
}
\end{minted}

\subsection{before()和after()}
在一些需要判断序号前后的地方出现了\mintinline{c}{before()}和\mintinline{c}{after()}
这两个函数。这两个函数的定义如下
\begin{minted}[linenos]{c}
/* include/net/tcp.h
 * 比较两个无符号32位整数
 */
static inline bool before(__u32 seq1, __u32 seq2)
{
        return (__s32)(seq1-seq2) < 0;
}
#define after(seq2, seq1)       before(seq1, seq2)
\end{minted}
可以看到，这两个函数实际上就是将两个数直接相减。之所以要单独弄个函数应该是为了避免强制转型
造成影响。序号都是32位无符号整型。

	\subsection{\mintinline{C}{tcp_shutdown}}

		\mintinline{C}{tcp_shutdown}是TCP的shutdown系统调用的传输层接口实现，由套接口层的实现\mintinline{C}{inet_shutdown}调用。
\begin{minted}[linenos]{C}
/*
Location:

	net/ipv4/tcp.c

Function:
	
	Shutdown the sending side of a connection. Much like close except
	that we don't receive shut down or sock_set_flag(sk, SOCK_DEAD).

Parameter:

	sk:传输控制块。
	how:
*/
void tcp_shutdown(struct sock *sk, int how)
{
	/*	We need to grab some memory, and put together a FIN,
	 *	and then put it into the queue to be sent.
	 *		Tim MacKenzie(tym@dibbler.cs.monash.edu.au) 4 Dec '92.
	 */
	if (!(how & SEND_SHUTDOWN))
		return;

	/* If we've already sent a FIN, or it's a closed state, skip this. */
	if ((1 << sk->sk_state) &
	    (TCPF_ESTABLISHED | TCPF_SYN_SENT |
	     TCPF_SYN_RECV | TCPF_CLOSE_WAIT)) {
		/* Clear out any half completed packets.  FIN if needed. */
		if (tcp_close_state(sk))
			tcp_send_fin(sk);
	}
}
\end{minted}
		如果是发送方向的关闭，并且TCP状态为ESTABLISHED、SYN\_SENT、SYN\_RECV或CLOSE\_WAIT时，根据TC状态迁移图和当前的状态设置新的状态，并在需要发送FIN时，调用FIN时，调用\mintinline{C}{tcp_send_fin}时向对方发送FIN。

		而对于接收方向的关闭，则无需向对方发送FIN，因为可能还需要向对方发送数据。至于接收方向的关闭的实现，在recvmsg系统调用中发现设置了RCV\_SHUTDOWN标志会立即返回。

	\subsection{\mintinline{C}{tcp_close}}
\begin{minted}[linenos]{C}
/*
Location:

	net/ipv4/tcp.c

Function:

		

Parameter:

	sk:传输控制块。
	timeout:在真正关闭控制块之前，可以发送剩余数据的时间。
	
*/
void tcp_close(struct sock *sk, long timeout)
{
	struct sk_buff *skb;
	int data_was_unread = 0;
	int state;

	lock_sock(sk);
	sk->sk_shutdown = SHUTDOWN_MASK;

	if (sk->sk_state == TCP_LISTEN) {
		tcp_set_state(sk, TCP_CLOSE);

		/* Special case. */
		inet_csk_listen_stop(sk);

		goto adjudge_to_death;
	}
\end{minted}

		首先，对传输控制块加锁。然后设置关闭标志为SHUTDOWN\_MASK,表示进行双向的关闭。

		如果套接口处于侦听状态，这种情况处理相对比较简单，因为没有建立起连接，因此无需发送FIN等操作。设置TCP的状态为CLOSE，然后终止侦听。最后跳转到adjudge\_to\_death处进行相关处理。

\begin{minted}[linenos]{C}
	/*  We need to flush the recv. buffs.  We do this only on the
	 *  descriptor close(什么意思), not protocol-sourced(什么意思) closes, because the
	 *  reader process may not have drained(消耗) the data yet!
	 */
	while ((skb = __skb_dequeue(&sk->sk_receive_queue)) != NULL) {
		u32 len = TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq;

		if (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)
			len--;
		data_was_unread += len;
		__kfree_skb(skb);
	}

	sk_mem_reclaim(sk);
\end{minted}

		因为是关闭连接，因此需要释放已接收队列上的段，同时统计释放了多少数据，然后回收缓存。

\begin{minted}[linenos]{C}
	/* If socket has been already reset (e.g. in tcp_reset()) - kill it. */
	if (sk->sk_state == TCP_CLOSE)
		goto adjudge_to_death;
\end{minted}

		如果socket本身就是close状态的话，直接跳到adjudge\_to\_death就好。

\begin{minted}[linenos]{C}
	/* As outlined in RFC 2525, section 2.17, we send a RST here because
	 * data was lost. To witness the awful effects of the old behavior of
	 * always doing a FIN, run an older 2.1.x kernel or 2.0.x, start a bulk
	 * GET in an FTP client, suspend the process, wait for the client to
	 * advertise a zero window, then kill -9 the FTP client, wheee...
	 * Note: timeout is always zero in such a case.
	 */
	if (unlikely(tcp_sk(sk)->repair)) {
		sk->sk_prot->disconnect(sk, 0);
	} else if (data_was_unread) {
		/* Unread data was tossed, zap the connection. */
		NET_INC_STATS_USER(sock_net(sk), LINUX_MIB_TCPABORTONCLOSE);
		tcp_set_state(sk, TCP_CLOSE);
		tcp_send_active_reset(sk, sk->sk_allocation);
	} else if (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime) {
		/* Check zero linger _after_ checking for unread data. */
		sk->sk_prot->disconnect(sk, 0);
		NET_INC_STATS_USER(sock_net(sk), LINUX_MIB_TCPABORTONDATA);
	} else if (tcp_close_state(sk)) {
		/* We FIN if the application ate all the data before
		 * zapping the connection.
		 */

		/* RED-PEN. Formally speaking, we have broken TCP state
		 * machine. State transitions:
		 *
		 * TCP_ESTABLISHED -> TCP_FIN_WAIT1
		 * TCP_SYN_RECV	-> TCP_FIN_WAIT1 (forget it, it's impossible)
		 * TCP_CLOSE_WAIT -> TCP_LAST_ACK
		 *
		 * are legal only when FIN has been sent (i.e. in window),
		 * rather than queued out of window. Purists blame.
		 *
		 * F.e. "RFC state" is ESTABLISHED,
		 * if Linux state is FIN-WAIT-1, but FIN is still not sent.
		 *
		 * The visible declinations are that sometimes
		 * we enter time-wait state, when it is not required really
		 * (harmless), do not send active resets, when they are
		 * required by specs (TCP_ESTABLISHED, TCP_CLOSE_WAIT, when
		 * they look as CLOSING or LAST_ACK for Linux)
		 * Probably, I missed some more holelets.
		 * 						--ANK
		 * XXX (TFO) - To start off we don't support SYN+ACK+FIN
		 * in a single packet! (May consider it later but will
		 * probably need API support or TCP_CORK SYN-ACK until
		 * data is written and socket is closed.)
		 */
		tcp_send_fin(sk);
	}

	sk_stream_wait_close(sk, timeout);
\end{minted}

\begin{minted}[linenos]{C}
adjudge_to_death:
	state = sk->sk_state;
	sock_hold(sk);
	sock_orphan(sk);

	/* It is the last release_sock in its life. It will remove backlog. */
	release_sock(sk);


	/* Now socket is owned by kernel and we acquire BH lock
	   to finish close. No need to check for user refs.
	 */
	local_bh_disable();
	.(sk);
	WARN_ON(sock_owned_by_user(sk));

	percpu_counter_inc(sk->sk_prot->orphan_count);

	/* Have we already been destroyed by a softirq or backlog? */
	if (state != TCP_CLOSE && sk->sk_state == TCP_CLOSE)
		goto out;

	/*	This is a (useful) BSD violating of the RFC. There is a
	 *	problem with TCP as specified in that the other end could
	 *	keep a socket open forever with no application left this end.
	 *	We use a 1 minute timeout (about the same as BSD) then kill
	 *	our end. If they send after that then tough - BUT: long enough
	 *	that we won't make the old 4*rto = almost no time - whoops
	 *	reset mistake.
	 *
	 *	Nope, it was not mistake. It is really desired behaviour
	 *	f.e. on http servers, when such sockets are useless, but
	 *	consume significant resources. Let's do it with special
	 *	linger2	option.					--ANK
	 */

	if (sk->sk_state == TCP_FIN_WAIT2) {
		struct tcp_sock *tp = tcp_sk(sk);
		if (tp->linger2 < 0) {
			tcp_set_state(sk, TCP_CLOSE);
			tcp_send_active_reset(sk, GFP_ATOMIC);
			NET_INC_STATS_BH(sock_net(sk),
					LINUX_MIB_TCPABORTONLINGER);
		} else {
			const int tmo = tcp_fin_time(sk);

			if (tmo > TCP_TIMEWAIT_LEN) {
				inet_csk_reset_keepalive_timer(sk,
						tmo - TCP_TIMEWAIT_LEN);
			} else {
				tcp_time_wait(sk, TCP_FIN_WAIT2, tmo);
				goto out;
			}
		}
	}
	if (sk->sk_state != TCP_CLOSE) {
		sk_mem_reclaim(sk);
		if (tcp_check_oom(sk, 0)) {
			tcp_set_state(sk, TCP_CLOSE);
			tcp_send_active_reset(sk, GFP_ATOMIC);
			NET_INC_STATS_BH(sock_net(sk),
					LINUX_MIB_TCPABORTONMEMORY);
		}
	}

	if (sk->sk_state == TCP_CLOSE) {
		struct request_sock *req = tcp_sk(sk)->fastopen_rsk;
		/* We could get here with a non-NULL req if the socket is
		 * aborted (e.g., closed with unread data) before 3WHS
		 * finishes.
		 */
		if (req)
			reqsk_fastopen_remove(sk, req, false);
		inet_csk_destroy_sock(sk);
	}
	/* Otherwise, socket is reprieved until protocol close. */

out:
	bh_unlock_sock(sk);
	local_bh_enable();
	sock_put(sk);
}
\end{minted}
