\chapter{非核心函数分析}

\minitoc

\section{SKB}

\section{Inet}
    \subsection{\mintinline{C}{inet_hash_connect && __inet_hash_connect}}

        \subsubsection{\mintinline{C}{inet_hash_connect}}
\begin{minted}[linenos]{C}
/*
 * Bind a port for a connect operation and hash it.
 */
int inet_hash_connect(struct inet_timewait_death_row *death_row,
              struct sock *sk)
{
    u32 port_offset = 0;

    if (!inet_sk(sk)->inet_num)
        port_offset = inet_sk_port_offset(sk);
    return __inet_hash_connect(death_row, sk, port_offset,
                   __inet_check_established);
}
\end{minted}

        \subsubsection{\mintinline{C}{__inet_hash_connect}}     
\begin{minted}[linenos]{C}
int __inet_hash_connect(struct inet_timewait_death_row *death_row,
        struct sock *sk, u32 port_offset,
        int (*check_established)(struct inet_timewait_death_row *,
            struct sock *, __u16, struct inet_timewait_sock **))
{
    struct inet_hashinfo *hinfo = death_row->hashinfo;
    const unsigned short snum = inet_sk(sk)->inet_num;
    struct inet_bind_hashbucket *head;
    struct inet_bind_bucket *tb;
    int ret;
    struct net *net = sock_net(sk);

    if (!snum) {
        int i, remaining, low, high, port;
        static u32 hint;
        u32 offset = hint + port_offset;
        struct inet_timewait_sock *tw = NULL;

        inet_get_local_port_range(net, &low, &high);
        remaining = (high - low) + 1;

        /* By starting with offset being an even number,
         * we tend to leave about 50% of ports for other uses,
         * like bind(0).
         */
        offset &= ~1;

        local_bh_disable();
        for (i = 0; i < remaining; i++) {
            port = low + (i + offset) % remaining;
            if (inet_is_local_reserved_port(net, port))
                continue;
            head = &hinfo->bhash[inet_bhashfn(net, port,
                    hinfo->bhash_size)];
            spin_lock(&head->lock);

            /* Does not bother with rcv_saddr checks,
             * because the established check is already
             * unique enough.
             */
            inet_bind_bucket_for_each(tb, &head->chain) {
                if (net_eq(ib_net(tb), net) &&
                    tb->port == port) {
                    if (tb->fastreuse >= 0 ||
                        tb->fastreuseport >= 0)
                        goto next_port;
                    WARN_ON(hlist_empty(&tb->owners));
                    if (!check_established(death_row, sk,
                                port, &tw))
                        goto ok;
                    goto next_port;
                }
            }

            tb = inet_bind_bucket_create(hinfo->bind_bucket_cachep,
                    net, head, port);
            if (!tb) {
                spin_unlock(&head->lock);
                break;
            }
            tb->fastreuse = -1;
            tb->fastreuseport = -1;
            goto ok;

        next_port:
            spin_unlock(&head->lock);
        }
        local_bh_enable();

        return -EADDRNOTAVAIL;

ok:
        hint += (i + 2) & ~1;

        /* Head lock still held and bh's disabled */
        inet_bind_hash(sk, tb, port);
        if (sk_unhashed(sk)) {
            inet_sk(sk)->inet_sport = htons(port);
            inet_ehash_nolisten(sk, (struct sock *)tw);
        }
        if (tw)
            inet_twsk_bind_unhash(tw, hinfo);
        spin_unlock(&head->lock);

        if (tw)
            inet_twsk_deschedule_put(tw);

        ret = 0;
        goto out;
    }

    head = &hinfo->bhash[inet_bhashfn(net, snum, hinfo->bhash_size)];
    tb  = inet_csk(sk)->icsk_bind_hash;
    spin_lock_bh(&head->lock);
    if (sk_head(&tb->owners) == sk && !sk->sk_bind_node.next) {
        inet_ehash_nolisten(sk, NULL);
        spin_unlock_bh(&head->lock);
        return 0;
    } else {
        spin_unlock(&head->lock);
        /* No definite answer... Walk to established hash table */
        ret = check_established(death_row, sk, snum, NULL);
out:
        local_bh_enable();
        return ret;
    }
}
\end{minted}

\subsection{\mintinline{c}{inet_twsk_put}}
该函数用于释放\mintinline{c}{inet_timewait_sock}结构体。
\begin{minted}[linenos]{c}
void inet_twsk_put(struct inet_timewait_sock *tw)
{
        /* 减小引用计数，如果计数为0，则释放它 */
        if (atomic_dec_and_test(&tw->tw_refcnt))
                inet_twsk_free(tw);
}
\end{minted}

\section{TCP层}

    \mintinline{c}{TCP_INC_STATS_BH}

    \mintinline{c}{rcu_read_unlock} 出现在\mintinline{c}{tcp_make_synack}中于MD5相关的部分。
    
    \mintinline{c}{net_xmit_eval}   定时器？？

\subsection{\mintinline{c}{__tcp_push_pending_frames}}
\begin{minted}[linenos]{c}
/* 将等待在队列中的包全部发出。 */
void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
                               int nonagle)
{
        /* 如果此时连接已经关闭了，那么直接返回。*/
        if (unlikely(sk->sk_state == TCP_CLOSE))
                return;

        /* 关闭nagle算法，将剩余的部分发送出去。 */
        if (tcp_write_xmit(sk, cur_mss, nonagle, 0,
                           sk_gfp_atomic(sk, GFP_ATOMIC)))
                tcp_check_probe_timer(sk);
}
\end{minted}

\subsection{\mintinline{c}{tcp_fin_time}}
计算等待接收FIN的超时时间。超时时间至少为$\frac{7}{2}$倍的rto。
\begin{minted}[linenos]{c}
static inline int tcp_fin_time(const struct sock *sk)
{
        int fin_timeout = tcp_sk(sk)->linger2 ? : sysctl_tcp_fin_timeout;
        const int rto = inet_csk(sk)->icsk_rto;

        if (fin_timeout < (rto << 2) - (rto >> 1))
                fin_timeout = (rto << 2) - (rto >> 1);

        return fin_timeout;
}
\end{minted}

\subsection{\mintinline{c}{tcp_done}}
\begin{minted}[linenos]{c}
/* 该函数用于完成关闭TCP连接，回收并清理相关资源。 */
void tcp_done(struct sock *sk)
{
        struct request_sock *req = tcp_sk(sk)->fastopen_rsk;

        /* 当套接字状态为SYN_SENT或SYN_RECV时，更新统计数据。 */
        if (sk->sk_state == TCP_SYN_SENT || sk->sk_state == TCP_SYN_RECV)
                TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_ATTEMPTFAILS);

        /* 将连接状态设置为关闭，并清除定时器。 */
        tcp_set_state(sk, TCP_CLOSE);
        tcp_clear_xmit_timers(sk);
        /* 当启用了Fast Open时，移除fastopen请求 */
        if (req)
                reqsk_fastopen_remove(sk, req, false);

        sk->sk_shutdown = SHUTDOWN_MASK;

        /* 如果状态不为SOCK_DEAD，则唤醒等待着的进程。 */
        if (!sock_flag(sk, SOCK_DEAD))
                sk->sk_state_change(sk);
        else
                inet_csk_destroy_sock(sk);
}
\end{minted}

\subsection{tcp\_init\_nondata\_skb}
该函数提供了初始化不含数据的skb的功能。函数原型如下：
\begin{minted}[linenos]{c}
tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags);
\end{minted}

\begin{description}
  \item[skb] 待初始化的\mintinline{c}{sk_buff}。
  \item[seq] 序号
  \item[flags] 标志位
\end{description}

\begin{minted}[linenos]{c}
static void tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags)
{
        /* 设置校验码 */
        skb->ip_summed = CHECKSUM_PARTIAL;
        skb->csum = 0;

        /* 设置标志位 */
        TCP_SKB_CB(skb)->tcp_flags = flags;
        TCP_SKB_CB(skb)->sacked = 0;

        tcp_skb_pcount_set(skb, 1);

        /* 设置起始序号 */
        TCP_SKB_CB(skb)->seq = seq;
        if (flags & (TCPHDR_SYN | TCPHDR_FIN))
                seq++;
        TCP_SKB_CB(skb)->end_seq = seq;
}
\end{minted}

\subsection{before()和after()}
在一些需要判断序号前后的地方出现了\mintinline{c}{before()}和\mintinline{c}{after()}
这两个函数。这两个函数的定义如下
\begin{minted}[linenos]{c}
/* include/net/tcp.h
 * 比较两个无符号32位整数
 */
static inline bool before(__u32 seq1, __u32 seq2)
{
        return (__s32)(seq1-seq2) < 0;
}
#define after(seq2, seq1)       before(seq1, seq2)
\end{minted}
可以看到，这两个函数实际上就是将两个数直接相减。之所以要单独弄个函数应该是为了避免强制转型
造成影响。序号都是32位无符号整型。

	\subsection{\mintinline{C}{tcp_shutdown}}

		\mintinline{C}{tcp_shutdown}是TCP的shutdown系统调用的传输层接口实现，由套接口层的实现\mintinline{C}{inet_shutdown}调用。
\begin{minted}[linenos]{C}
/*
Location:

	net/ipv4/tcp.c

Function:
	
	Shutdown the sending side of a connection. Much like close except
	that we don't receive shut down or sock_set_flag(sk, SOCK_DEAD).

Parameter:

	sk:传输控制块。
	how:
*/
void tcp_shutdown(struct sock *sk, int how)
{
	/*	We need to grab some memory, and put together a FIN,
	 *	and then put it into the queue to be sent.
	 *		Tim MacKenzie(tym@dibbler.cs.monash.edu.au) 4 Dec '92.
	 */
	if (!(how & SEND_SHUTDOWN))
		return;

	/* If we've already sent a FIN, or it's a closed state, skip this. */
	if ((1 << sk->sk_state) &
	    (TCPF_ESTABLISHED | TCPF_SYN_SENT |
	     TCPF_SYN_RECV | TCPF_CLOSE_WAIT)) {
		/* Clear out any half completed packets.  FIN if needed. */
		if (tcp_close_state(sk))
			tcp_send_fin(sk);
	}
}
\end{minted}
		如果是发送方向的关闭，并且TCP状态为ESTABLISHED、SYN\_SENT、SYN\_RECV或CLOSE\_WAIT时，根据TC状态迁移图和当前的状态设置新的状态，并在需要发送FIN时，调用FIN时，调用\mintinline{C}{tcp_send_fin}时向对方发送FIN。

		而对于接收方向的关闭，则无需向对方发送FIN，因为可能还需要向对方发送数据。至于接收方向的关闭的实现，在recvmsg系统调用中发现设置了RCV\_SHUTDOWN标志会立即返回。

	\subsection{\mintinline{C}{tcp_close}}
\begin{minted}[linenos]{C}
/*
Location:

	net/ipv4/tcp.c

Function:

		

Parameter:

	sk:传输控制块。
	timeout:在真正关闭控制块之前，可以发送剩余数据的时间。
	
*/
void tcp_close(struct sock *sk, long timeout)
{
	struct sk_buff *skb;
	int data_was_unread = 0;
	int state;

	lock_sock(sk);
	sk->sk_shutdown = SHUTDOWN_MASK;

	if (sk->sk_state == TCP_LISTEN) {
		tcp_set_state(sk, TCP_CLOSE);

		/* Special case. */
		inet_csk_listen_stop(sk);

		goto adjudge_to_death;
	}
\end{minted}

		首先，对传输控制块加锁。然后设置关闭标志为SHUTDOWN\_MASK,表示进行双向的关闭。

		如果套接口处于侦听状态，这种情况处理相对比较简单，因为没有建立起连接，因此无需发送FIN等操作。设置TCP的状态为CLOSE，然后终止侦听。最后跳转到adjudge\_to\_death处进行相关处理。

\begin{minted}[linenos]{C}
	/*  We need to flush the recv. buffs.  We do this only on the
	 *  descriptor close(什么意思), not protocol-sourced(什么意思) closes, because the
	 *  reader process may not have drained(消耗) the data yet!
	 */
	while ((skb = __skb_dequeue(&sk->sk_receive_queue)) != NULL) {
		u32 len = TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq;

		if (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)
			len--;
		data_was_unread += len;
		__kfree_skb(skb);
	}

	sk_mem_reclaim(sk);
\end{minted}

		因为是关闭连接，因此需要释放已接收队列上的段，同时统计释放了多少数据，然后回收缓存。

\begin{minted}[linenos]{C}
	/* If socket has been already reset (e.g. in tcp_reset()) - kill it. */
	if (sk->sk_state == TCP_CLOSE)
		goto adjudge_to_death;
\end{minted}

		如果socket本身就是close状态的话，直接跳到adjudge\_to\_death就好。

\begin{minted}[linenos]{C}
	/* As outlined in RFC 2525, section 2.17, we send a RST here because
	 * data was lost. To witness the awful effects of the old behavior of
	 * always doing a FIN, run an older 2.1.x kernel or 2.0.x, start a bulk
	 * GET in an FTP client, suspend the process, wait for the client to
	 * advertise a zero window, then kill -9 the FTP client, wheee...
	 * Note: timeout is always zero in such a case.
	 */
	if (unlikely(tcp_sk(sk)->repair)) {
		sk->sk_prot->disconnect(sk, 0);
	} else if (data_was_unread) {
		/* Unread data was tossed, zap the connection. */
		NET_INC_STATS_USER(sock_net(sk), LINUX_MIB_TCPABORTONCLOSE);
		tcp_set_state(sk, TCP_CLOSE);
		tcp_send_active_reset(sk, sk->sk_allocation);
	} else if (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime) {
		/* Check zero linger _after_ checking for unread data. */
		sk->sk_prot->disconnect(sk, 0);
		NET_INC_STATS_USER(sock_net(sk), LINUX_MIB_TCPABORTONDATA);
	} else if (tcp_close_state(sk)) {
		/* We FIN if the application ate all the data before
		 * zapping the connection.
		 */

		/* RED-PEN. Formally speaking, we have broken TCP state
		 * machine. State transitions:
		 *
		 * TCP_ESTABLISHED -> TCP_FIN_WAIT1
		 * TCP_SYN_RECV	-> TCP_FIN_WAIT1 (forget it, it's impossible)
		 * TCP_CLOSE_WAIT -> TCP_LAST_ACK
		 *
		 * are legal only when FIN has been sent (i.e. in window),
		 * rather than queued out of window. Purists blame.
		 *
		 * F.e. "RFC state" is ESTABLISHED,
		 * if Linux state is FIN-WAIT-1, but FIN is still not sent.
		 *
		 * The visible declinations are that sometimes
		 * we enter time-wait state, when it is not required really
		 * (harmless), do not send active resets, when they are
		 * required by specs (TCP_ESTABLISHED, TCP_CLOSE_WAIT, when
		 * they look as CLOSING or LAST_ACK for Linux)
		 * Probably, I missed some more holelets.
		 * 						--ANK
		 * XXX (TFO) - To start off we don't support SYN+ACK+FIN
		 * in a single packet! (May consider it later but will
		 * probably need API support or TCP_CORK SYN-ACK until
		 * data is written and socket is closed.)
		 */
		tcp_send_fin(sk);
	}

	sk_stream_wait_close(sk, timeout);
\end{minted}

\begin{minted}[linenos]{C}
adjudge_to_death:
	state = sk->sk_state;
	sock_hold(sk);
	sock_orphan(sk);

	/* It is the last release_sock in its life. It will remove backlog. */
	release_sock(sk);


	/* Now socket is owned by kernel and we acquire BH lock
	   to finish close. No need to check for user refs.
	 */
	local_bh_disable();
	.(sk);
	WARN_ON(sock_owned_by_user(sk));

	percpu_counter_inc(sk->sk_prot->orphan_count);

	/* Have we already been destroyed by a softirq or backlog? */
	if (state != TCP_CLOSE && sk->sk_state == TCP_CLOSE)
		goto out;

	/*	This is a (useful) BSD violating of the RFC. There is a
	 *	problem with TCP as specified in that the other end could
	 *	keep a socket open forever with no application left this end.
	 *	We use a 1 minute timeout (about the same as BSD) then kill
	 *	our end. If they send after that then tough - BUT: long enough
	 *	that we won't make the old 4*rto = almost no time - whoops
	 *	reset mistake.
	 *
	 *	Nope, it was not mistake. It is really desired behaviour
	 *	f.e. on http servers, when such sockets are useless, but
	 *	consume significant resources. Let's do it with special
	 *	linger2	option.					--ANK
	 */

	if (sk->sk_state == TCP_FIN_WAIT2) {
		struct tcp_sock *tp = tcp_sk(sk);
		if (tp->linger2 < 0) {
			tcp_set_state(sk, TCP_CLOSE);
			tcp_send_active_reset(sk, GFP_ATOMIC);
			NET_INC_STATS_BH(sock_net(sk),
					LINUX_MIB_TCPABORTONLINGER);
		} else {
			const int tmo = tcp_fin_time(sk);

			if (tmo > TCP_TIMEWAIT_LEN) {
				inet_csk_reset_keepalive_timer(sk,
						tmo - TCP_TIMEWAIT_LEN);
			} else {
				tcp_time_wait(sk, TCP_FIN_WAIT2, tmo);
				goto out;
			}
		}
	}
	if (sk->sk_state != TCP_CLOSE) {
		sk_mem_reclaim(sk);
		if (tcp_check_oom(sk, 0)) {
			tcp_set_state(sk, TCP_CLOSE);
			tcp_send_active_reset(sk, GFP_ATOMIC);
			NET_INC_STATS_BH(sock_net(sk),
					LINUX_MIB_TCPABORTONMEMORY);
		}
	}

	if (sk->sk_state == TCP_CLOSE) {
		struct request_sock *req = tcp_sk(sk)->fastopen_rsk;
		/* We could get here with a non-NULL req if the socket is
		 * aborted (e.g., closed with unread data) before 3WHS
		 * finishes.
		 */
		if (req)
			reqsk_fastopen_remove(sk, req, false);
		inet_csk_destroy_sock(sk);
	}
	/* Otherwise, socket is reprieved until protocol close. */

out:
	bh_unlock_sock(sk);
	local_bh_enable();
	sock_put(sk);
}
\end{minted}
